<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>k8s调度之亲和性与反亲和性</title>
      <link href="/2025/07/10/k8s-qin-he-xing-he-fan-qin-he-xing/"/>
      <url>/2025/07/10/k8s-qin-he-xing-he-fan-qin-he-xing/</url>
      
        <content type="html"><![CDATA[<h2 id="k8s亲和性与反亲和性"><a href="#k8s亲和性与反亲和性" class="headerlink" title="k8s亲和性与反亲和性"></a>k8s亲和性与反亲和性</h2><p>Kubernetes 的亲和性和反亲和性是用于控制 Pod 调度策略的强大工具。它允许根据节点或其他 Pod 的属性来指定 Pod 应该或不应该被调度到哪些节点上。通常情况下Pod被分配到哪些Node是不需要我们操心的，这个过程会由scheduler自动实现。但有时，我们需要让Pod按照我们的预想运行在Node上，例如某些应用 “必须/或者尽量” 跑在具有SSD存储的节点上；有些彼此相关的Pod应用应该跑在同一个节点上；有些业务Pod为了实现高可用需要将多个副本分散到不同的主机上，使每个主机有且只能运行服务的一个副本。为此，k8s为我们提供了这样的策略，我们可以通过使用 “亲和性/反亲和性” 制定一些规则来实现我们的需求</p><h3 id="亲和性（Affinity）"><a href="#亲和性（Affinity）" class="headerlink" title="亲和性（Affinity）"></a>亲和性（Affinity）</h3><p>亲和性用于指定 Pod 应该被调度到哪些节点上。可以基于节点的标签、节点的拓扑结构（例如，可用区或机架）或其他 Pod 的标签。</p><h4 id="亲和性类型"><a href="#亲和性类型" class="headerlink" title="亲和性类型:"></a>亲和性类型:</h4><ul><li><p>节点亲和性（Node Affinity）: 基于节点的标签或拓扑结构来指定 Pod 应该被调度到哪些节点上。Pod 与 Node 之间的匹配规则。</p></li><li><p>Pod 亲和性（Pod Affinity）: 基于其他 Pod 的标签来指定 Pod 应该被调度到哪些节点上。Pod 与 Pod 之间的匹配规则。</p></li></ul><h3 id="反亲和性（Anti-affinity）"><a href="#反亲和性（Anti-affinity）" class="headerlink" title="反亲和性（Anti-affinity）"></a>反亲和性（Anti-affinity）</h3><p>反亲和性用于指定 Pod 不应该被调度到哪些节点上。可以基于节点的标签、节点的拓扑结构或其他 Pod 的标签。</p><h4 id="反亲和性类型"><a href="#反亲和性类型" class="headerlink" title="反亲和性类型:"></a>反亲和性类型:</h4><ul><li><p>节点反亲和性（Node Anti-affinity）: 基于节点的标签或拓扑结构来指定 Pod 不应该被调度到哪些节点上。</p></li><li><p>Pod 反亲和性（Pod Anti-affinity）: 基于其他 Pod 的标签来指定 Pod 不应该被调度到哪些节点上。<strong>Pod 与 Pod 不能在一起的规则</strong> 。</p></li></ul><h3 id="亲和性调度："><a href="#亲和性调度：" class="headerlink" title="亲和性调度："></a>亲和性调度：</h3><ul><li><p>硬策略：可以理解为必须遵守的规则，就是如果没有满足条件的节点的话，就不断重试直到满足条件为止。对应的配置规则为 <strong>requiredDuringSchedulingIgnoredDuringExecution</strong>。</p></li><li><p>软策略：可以理解为尽量遵守的规则，就是如果现在没有满足调度要求的节点的话，pod就会忽略这条规则，继续完成调度的过程，说白了就是满足条件最好了，没有的话也无所谓。对应的配置规则为 <strong>preferredDuringSchedulingIgnoredDuringExecution</strong>。</p></li></ul><p><strong>所有的规则以 Label 作为元数据</strong>。对于 label 的匹配规则，可选的操作符有：</p><table><thead><tr><th>操作符</th><th>规则说明</th></tr></thead><tbody><tr><td>In</td><td>label 的值在某个列表中</td></tr><tr><td>NotIn</td><td>label 的值不在某个列表中</td></tr><tr><td>Exists</td><td>某个 label 存在</td></tr><tr><td>DoesNotExist</td><td>某个 label 不存在</td></tr><tr><td>Gt</td><td>label 的值大于某个值（字符串比较）</td></tr><tr><td>Lt</td><td>label 的值小于某个值（字符串比较）</td></tr></tbody></table><p>如果 nodeAffinity 中 nodeSelectorTerms 有多个选项，则节点满足任何一个条件就可以；</p><p>如果 matchExpressions 有多个选项，则只有同时满足这些逻辑选项的节点才能运行 Pod。</p><h3 id="亲和性和反亲和性作用范围："><a href="#亲和性和反亲和性作用范围：" class="headerlink" title="亲和性和反亲和性作用范围："></a>亲和性和反亲和性作用范围：</h3><p>在 Kubernetes 的亲和性和反亲和性配置中，topologyKey 用于指定拓扑域，决定了亲和性或反亲和性规则的作用范围。</p><h4 id="topologyKey-的作用"><a href="#topologyKey-的作用" class="headerlink" title="topologyKey 的作用:"></a>topologyKey 的作用:</h4><ul><li><p>定义拓扑域: topologyKey 指定了用于匹配节点或 Pod 的拓扑域。例如，kubernetes.io/hostname 表示节点，failure-domain.beta.kubernetes.io/zone 表示可用区。</p></li><li><p>限制亲和性/反亲和性范围: topologyKey 限制了亲和性或反亲和性规则的应用范围。例如，如果 topologyKey 设置为 kubernetes.io/hostname，则亲和性或反亲和性规则只会在同一个节点上生效。</p></li></ul><h4 id="topologyKey-的常见值"><a href="#topologyKey-的常见值" class="headerlink" title="topologyKey 的常见值:"></a>topologyKey 的常见值:</h4><ul><li><p>kubernetes.io/hostname: 表示节点。</p></li><li><p>failure-domain.beta.kubernetes.io/zone: 表示可用区。</p></li><li><p>failure-domain.beta.kubernetes.io/region: 表示区域。</p></li><li><p>topology.kubernetes.io/region: 表示区域（与 failure-domain.beta.kubernetes.io/region 相同）。</p></li><li><p>topology.kubernetes.io/zone: 表示可用区（与 failure-domain.beta.kubernetes.io/zone 相同）。</p></li></ul><h4 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景:"></a>使用场景:</h4><ul><li><p>亲和性:</p><ul><li><p>将特定类型的 Pod 调度到具有特定资源或功能的节点上。</p></li><li><p>将 Pod 调度到同一可用区或机架上的节点上，以提高网络性能。</p></li><li><p>将 Pod 调度到与其他 Pod 相同的节点上，以实现数据共享或协同工作。</p></li></ul></li><li><p>反亲和性:</p><ul><li><p>将 Pod 分散到不同的节点上，以提高可用性和容错性。</p></li><li><p>将 Pod 分散到不同的可用区或机架上，以提高容错性和地理冗余。</p></li><li><p>将 Pod 分散到不同的节点上，以避免资源竞争或性能冲突。</p></li></ul></li></ul><h4 id="配置"><a href="#配置" class="headerlink" title="配置:"></a>配置:</h4><p>亲和性和反亲和性可以通过 Pod 的 affinity 字段进行配置。</p><p>示例:</p><h5 id="节点亲和性"><a href="#节点亲和性" class="headerlink" title="节点亲和性:"></a>节点亲和性:</h5><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>pod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>    <span class="token key atrule">nodeAffinity</span><span class="token punctuation">:</span>      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>        <span class="token key atrule">nodeSelectorTerms</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> disktype            <span class="token key atrule">operator</span><span class="token punctuation">:</span> In            <span class="token key atrule">values</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> ssd</code></pre><p>这个配置指定 Pod 必须被调度到具有标签 disktype: ssd 的节点上。</p><h5 id="Pod-反亲和性"><a href="#Pod-反亲和性" class="headerlink" title="Pod 反亲和性:"></a>Pod 反亲和性:</h5><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>pod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>    <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>          <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app            <span class="token key atrule">operator</span><span class="token punctuation">:</span> In            <span class="token key atrule">values</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> my<span class="token punctuation">-</span>app        <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname</code></pre><p>这个配置指定 Pod 不能与具有标签 app: my-app 的其他 Pod 在同一个节点上运行。</p><h3 id="K8s-pod-反亲和性配置策略由“必须”改为“尽量”"><a href="#K8s-pod-反亲和性配置策略由“必须”改为“尽量”" class="headerlink" title="K8s pod 反亲和性配置策略由“必须”改为“尽量”"></a>K8s pod 反亲和性配置策略由“必须”改为“尽量”</h3><p>在 Kubernetes 中，Pod 反亲和性（podAntiAffinity）用于防止 Pod 被调度到具有特定标签的节点上。默认情况下，反亲和性规则是“必须”遵守的，这意味着如果找不到符合规则的节点，Pod 将无法被调度。</p><p>要将反亲和性策略由“必须”改为“尽量”，需要修改 Pod 定义中的 podAntiAffinity 配置。</p><p>以下是具体的步骤：</p><ol><li><p>找到 podAntiAffinity 配置: 在 Pod 的 YAML 文件中，找到 spec.affinity.podAntiAffinity 部分。</p></li><li><p>修改 topologyKey:</p><ul><li><p>确认 topologyKey 设置为合适的拓扑域，例如 kubernetes.io/hostname（表示节点）或 failure-domain.beta.kubernetes.io/zone（表示可用区）。</p></li><li><p>topologyKey 决定了反亲和性规则的作用范围。</p></li></ul></li><li><p>将 requiredDuringSchedulingIgnoredDuringExecution 改为 preferredDuringSchedulingIgnoredDuringExecution:</p><ul><li><p>requiredDuringSchedulingIgnoredDuringExecution 表示调度器必须遵守反亲和性规则。</p></li><li><p>preferredDuringSchedulingIgnoredDuringExecution 表示调度器会尽量遵守反亲和性规则，但如果找不到符合条件的节点，仍然会调度 Pod。</p></li></ul></li></ol><h4 id="示例"><a href="#示例" class="headerlink" title="示例:"></a>示例:</h4><p>假设有一个 Pod 定义如下：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>pod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>    <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>      <span class="token key atrule">requiredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>          <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>          <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app            <span class="token key atrule">operator</span><span class="token punctuation">:</span> In            <span class="token key atrule">values</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> my<span class="token punctuation">-</span>app        <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname</code></pre><p>这个配置要求 Pod 不能与具有标签 app: my-app 的其他 Pod 在同一个节点上运行。</p><p><strong>要将策略改为“尽量”，可以将 requiredDuringSchedulingIgnoredDuringExecution 改为 preferredDuringSchedulingIgnoredDuringExecution：</strong></p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> my<span class="token punctuation">-</span>pod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">affinity</span><span class="token punctuation">:</span>    <span class="token key atrule">podAntiAffinity</span><span class="token punctuation">:</span>      <span class="token key atrule">preferredDuringSchedulingIgnoredDuringExecution</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">weight</span><span class="token punctuation">:</span> <span class="token number">100</span>        <span class="token key atrule">podAffinityTerm</span><span class="token punctuation">:</span>          <span class="token key atrule">labelSelector</span><span class="token punctuation">:</span>            <span class="token key atrule">matchExpressions</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">key</span><span class="token punctuation">:</span> app              <span class="token key atrule">operator</span><span class="token punctuation">:</span> In              <span class="token key atrule">values</span><span class="token punctuation">:</span>              <span class="token punctuation">-</span> my<span class="token punctuation">-</span>app          <span class="token key atrule">topologyKey</span><span class="token punctuation">:</span> kubernetes.io/hostname</code></pre><p>修改后，调度器会尽量将 Pod 调度到没有运行 app: my-app Pod 的节点上。但如果所有节点都运行了 app: my-app Pod，调度器仍然会将 Pod 调度到其中一个节点上。</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意:"></a>注意:</h4><ul><li><p>preferredDuringSchedulingIgnoredDuringExecution 需要设置 weight 参数，用于表示优先级，值越大表示优先级越高。</p></li><li><p>修改后的策略并不能完全保证 Pod 不会被调度到同一个节点上，只能说是尽量避免。</p></li></ul><h3 id="k8s-pod-pending-报错：xxx-node-s-didn’t-match-pod-affinity-affinity-xxx-node-s-didn’t-match-pod-anti-affinity-rules-问题解析"><a href="#k8s-pod-pending-报错：xxx-node-s-didn’t-match-pod-affinity-affinity-xxx-node-s-didn’t-match-pod-anti-affinity-rules-问题解析" class="headerlink" title="k8s pod pending 报错：xxx node(s) didn’t match pod affinity/affinity, xxx node(s) didn’t match pod anti-affinity rules. 问题解析"></a>k8s pod pending 报错：xxx node(s) didn’t match pod affinity/affinity, xxx node(s) didn’t match pod anti-affinity rules. 问题解析</h3><p>这个错误信息表明 Kubernetes 集群中的 xxx 个节点都无法满足 Pod 的亲和性或反亲和性规则，导致 Pod 处于 Pending 状态。</p><h4 id="导致此错误的常见原因包括："><a href="#导致此错误的常见原因包括：" class="headerlink" title="导致此错误的常见原因包括："></a>导致此错误的常见原因包括：</h4><ol><li><p>资源不足: 节点可能没有足够的 CPU、内存或其他资源来运行 Pod。</p></li><li><p>污点和容忍度不匹配: 节点可能具有 Pod 不容忍的污点（Taint）。</p></li><li><p>亲和性/反亲和性规则过于严格: Pod 可能设置了过于严格的亲和性或反亲和性规则，导致无法找到合适的节点。</p></li><li><p>节点选择器冲突: Pod 的节点选择器可能与亲和性/反亲和性规则冲突。</p></li><li><p>节点亲和性冲突: Pod 的节点亲和性规则可能相互冲突，导致无法找到合适的节点。</p></li></ol><h4 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤:"></a>解决步骤:</h4><ol><li><p>检查资源限制和节点容量:</p><ul><li><p>使用 kubectl describe node &lt;节点名称&gt; 检查节点的资源容量。</p></li><li><p>使用 kubectl describe pod &lt;pod名称&gt; 检查 Pod 的资源请求和限制。</p></li><li><p>确保节点有足够的可用资源来满足 Pod 的需求。</p></li></ul></li><li><p>检查污点和容忍度:</p><ul><li><p>使用 kubectl describe node &lt;节点名称&gt; 检查节点上的污点。</p></li><li><p>使用 kubectl describe pod &lt;pod名称&gt; 检查 Pod 的容忍度设置。</p></li><li><p>确保 Pod 容忍所有节点上的污点，或者修改污点和容忍度设置以使其匹配。</p></li></ul></li><li><p>审查亲和性/反亲和性规则:</p><ul><li><p>检查 Pod 定义中的 affinity 和 antiAffinity 字段。</p></li><li><p>确保规则不会过于严格，例如要求 Pod 必须与特定标签的 Pod 位于同一节点，但该标签的 Pod 数量不足。</p></li><li><p>尝试使用 preferredDuringSchedulingIgnoredDuringExecution 代替 requiredDuringSchedulingIgnoredDuringExecution，以便调度器在找不到完全匹配的节点时可以做出妥协。</p></li></ul></li><li><p>检查节点选择器:</p><ul><li><p>检查 Pod 定义中的 nodeSelector 字段。</p></li><li><p>确保节点选择器不会亲和性/反亲和性规则冲突。</p></li></ul></li><li><p>检查事件日志:</p><ul><li>使用 kubectl describe pod &lt;pod名称&gt; 查看 Pod 的事件日志，以获取有关调度失败的更多详细信息。</li></ul></li></ol><h4 id="其他建议"><a href="#其他建议" class="headerlink" title="其他建议:"></a>其他建议:</h4><ul><li><p>尝试删除并重新创建 Pod，看看问题是否解决。</p></li><li><p>尝试使用 kubectl cordon &lt;节点名称&gt; 将节点标记为不可调度，然后使用 kubectl uncordon &lt;节点名称&gt; 重新启用调度，看看是否可以解决问题。</p></li><li><p>如果使用的是自定义调度器，请检查其配置是否正确。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Kubernetes </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes集群问题排查指南</title>
      <link href="/2025/04/02/kubernetes-ji-qun-wen-ti-pai-cha-zhi-nan/"/>
      <url>/2025/04/02/kubernetes-ji-qun-wen-ti-pai-cha-zhi-nan/</url>
      
        <content type="html"><![CDATA[<p>Kubernetes集群组件较多，当集群出现某个问题的时候需要从多方面排查，下面这张图梳理常见异常问题的排查思路，可以以参考此图进行排查。</p><p><img src="/image.png" alt="PNG"></p><p>pdf文件下载地址：<a href="https://static.learnk8s.io/168db7d27bbf0e31a0bd038bf98757fd.pdf">Kubernets集群问题排查指南.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> TroubleShooting </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从k8s容器丢包事件中掌握内核参数优化技巧</title>
      <link href="/2024/07/03/cong-k8s-rong-qi-diu-bao-shi-jian-zhong-zhang-wo-nei-he-can-shu-you-hua-ji-qiao/"/>
      <url>/2024/07/03/cong-k8s-rong-qi-diu-bao-shi-jian-zhong-zhang-wo-nei-he-can-shu-you-hua-ji-qiao/</url>
      
        <content type="html"><![CDATA[<h1 id="从k8s容器丢包事件中掌握内核参数优化技巧"><a href="#从k8s容器丢包事件中掌握内核参数优化技巧" class="headerlink" title="从k8s容器丢包事件中掌握内核参数优化技巧"></a>从k8s容器丢包事件中掌握内核参数优化技巧</h1><p>在k8s的使用场景中，容器不是仅仅能运行就算ok，往往还需要进行容器的内核参数优化和应用程序参数的调优，如在高并发的业务场景下，运行一个java程序，我们不仅需要对其JVM参数进行调优，而且需要对其所在的容器进行内核参数优化，这篇文章主要通过一次容器丢包事件介绍容器中内核参数优化的方法。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>线上业务反馈接口偶发性返回502状态码，从请求日志和监控系统看，有好几个微服务的接口都有出现，期间容器资源使用率平稳并无异常，而502的发生主要聚焦在业务高峰阶段，所以暂时排除了是微服务性能问题，优先尝试通过压测的方式在非生产环境复现。</p><h2 id="请求链路"><a href="#请求链路" class="headerlink" title="请求链路"></a>请求链路</h2><p>为了方便复现，将pod设置为单副本，链路大致如下：</p><p>client –&gt; ingress –&gt; service –&gt; pod</p><h2 id="排查与分析过程"><a href="#排查与分析过程" class="headerlink" title="排查与分析过程"></a>排查与分析过程</h2><p>1、使用ab压测，设置请求数50000，并发数200，未能复现；</p><p>2、尝试增加并发数至500时，502的场景复现了，起初怀疑是conntrack table满了，但通过dmesg和conntrack -S查看并无异常；</p><p>3、经过一顿排查，最终通过下面的指令定位到容器的半连接队列满了，导致请求被丢弃</p><pre class=" language-sh"><code class="language-sh">netstat -st | egrep -i "drop|reject|overflowed|listen|filter|TCPSYNChallenge"</code></pre><p><img src="/2024/07/03/cong-k8s-rong-qi-diu-bao-shi-jian-zhong-zhang-wo-nei-he-can-shu-you-hua-ji-qiao/image.png" alt="Alt text"></p><h2 id="知识点插播"><a href="#知识点插播" class="headerlink" title="知识点插播"></a>知识点插播</h2><p>1、排查与分析过程中用到的netstat指令的作用是什么？</p><p>打印包含上述关键词相关的 TCP 统计信息，帮助我们快速定位和分析网络问题，如丢包、连接被拒绝、缓冲区溢出、监听端口状态、包过滤情况以及 TCP SYN 挑战等。</p><ul><li><p>drop：表示丢弃的数据包，可能由于网络拥塞或错误配置导致。</p></li><li><p>reject：表示被拒绝的数据包，通常是因为防火墙规则或访问控制列表（ACL）拒绝了这些连接。</p></li><li><p>overflowed：表示溢出的数据包，可能是由于缓冲区或队列满了，导致无法处理更多的数据包。</p></li><li><p>listen：表示监听的端口和连接，通常用于显示服务器正在监听的端口和等待连接的数量。</p></li><li><p>filter：表示过滤的统计信息，通常与防火墙或包过滤器相关。</p></li><li><p>TCPSYNChallenge：表示 TCP SYN 挑战包的统计信息，用于防止 SYN flood 攻击的一种保护机制。</p></li></ul><p>2、什么是半连接队列/长连接队列？二者之间的区别又在哪？</p><p>如图所示，一个完整的TCP连接的建立，会经历一次3次握手，两个状态：SYN_REVD、ESTABELLISHED，而操作系统中用来存放他们的，我们称之为队列，即</p><ul><li><p>半连接队列：存放SYN的队列</p></li><li><p>全连接队列：存放已经完成连接的队列</p></li></ul><p><img src="/2024/07/03/cong-k8s-rong-qi-diu-bao-shi-jian-zhong-zhang-wo-nei-he-can-shu-you-hua-ji-qiao/image-1.png" alt="Alt text"></p><p>3、半连接队列/长连接队列的长度跟哪些内核参数有关？</p><ul><li><p>半连接队列：长度为net.core.somaxconn、tcp_max_syn_backlog的最小值</p></li><li><p>全连接队列：长度为net.core.somaxconn的值。</p></li></ul><p>其中tcp_max_syn_backlog的值并未定义，而net.core.somaxconn的默认值为128，一般我们可以调整net.core.somaxconn的值来增加队列的长度。</p><h2 id="问题优化"><a href="#问题优化" class="headerlink" title="问题优化"></a>问题优化</h2><p>在Pod中并不能直接通过sysctl修改内核参数，所以我们使用init容器进行修改，在yaml中加入如下内容，将连接队列长度调整为2048，在优化后502状态码明显减少。</p><pre class=" language-yaml"><code class="language-yaml">    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">initContainers</span><span class="token punctuation">:</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> init<span class="token punctuation">-</span>sysctl          <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox          <span class="token key atrule">command</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> sh            <span class="token punctuation">-</span> <span class="token string">'-c'</span>            <span class="token punctuation">-</span> echo 2048 <span class="token punctuation">></span> /proc/sys/net/core/somaxconn          <span class="token key atrule">resources</span><span class="token punctuation">:</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>          <span class="token key atrule">terminationMessagePath</span><span class="token punctuation">:</span> /dev/termination<span class="token punctuation">-</span>log          <span class="token key atrule">terminationMessagePolicy</span><span class="token punctuation">:</span> File          <span class="token key atrule">imagePullPolicy</span><span class="token punctuation">:</span> Always          <span class="token key atrule">securityContext</span><span class="token punctuation">:</span>            <span class="token key atrule">privileged</span><span class="token punctuation">:</span> <span class="token boolean important">true</span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上所述，容器的性能并不能简单的交给弹性伸缩，一味地增加资源有时候并不能解决问题，反而会造成资源浪费，所以容器的性能和应用程序的性能我们都应该去关注和优化，这期的分享就到这里，谢谢！</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> 容器 </tag>
            
            <tag> 性能调优 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes中CPU和内存使用限制</title>
      <link href="/2024/06/20/kubernetes-zhong-cpu-he-nei-cun-shi-yong-xian-zhi/"/>
      <url>/2024/06/20/kubernetes-zhong-cpu-he-nei-cun-shi-yong-xian-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="Kubernetes中CPU和内存使用限制与监控"><a href="#Kubernetes中CPU和内存使用限制与监控" class="headerlink" title="Kubernetes中CPU和内存使用限制与监控"></a>Kubernetes中CPU和内存使用限制与监控</h1><p>容器的内存和 CPU 资源是最应该重视的两个资源，我之前所在的公司就因为容器 CPU Throttling 问题导致 Redis 集群故障，越是延迟敏感型应用越是要注意。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>使用 Kubernetes 时，内存不足（OOM）错误和 CPU 限制（Throttling）是云应用程序中资源处理的主要难题。为什么呢？</p><p>云应用程序中的 CPU 和内存要求变得越来越重要，因为它们与您的云成本直接相关。</p><p>通过 limits 和 requests，您可以配置 pod 应如何分配内存和 CPU 资源，以防止资源匮乏并调整云成本。</p><p>如果节点没有足够的资源，Pod 可能会因抢占或节点压力而被驱逐。</p><p>当进程运行内存不足 (OOM) 时，它会因为没有所需的资源而被 Kill。</p><p>如果 CPU 消耗高于实际 limits，进程将开始受到限制。</p><p>OK，如何监控 Pod 快要 OOM 了，或者 CPU 快要被限制了呢？</p><h2 id="Kubernetes-OOM"><a href="#Kubernetes-OOM" class="headerlink" title="Kubernetes OOM"></a>Kubernetes OOM</h2><p>Pod 中的每个容器都需要内存才能运行。</p><p>Kubernetes limits 是在 Pod 定义或 Deployment 定义中为每个容器设置的。</p><p>所有现代 Unix 系统都有一种方法可以杀死进程，以此回收内存（没用空闲内存的时候，只能杀进程了）。这个错误将被标记为 137 错误码或 OOMKilled。</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">State</span><span class="token punctuation">:</span>          Running    <span class="token key atrule">Started</span><span class="token punctuation">:</span>      Thu<span class="token punctuation">,</span> 10 Oct 2019 11<span class="token punctuation">:</span>14<span class="token punctuation">:</span>13 +0200Last State<span class="token punctuation">:</span>     Terminated    <span class="token key atrule">Reason</span><span class="token punctuation">:</span>       OOMKilled    Exit Code<span class="token punctuation">:</span>    <span class="token number">137</span>    <span class="token key atrule">Started</span><span class="token punctuation">:</span>      Thu<span class="token punctuation">,</span> 10 Oct 2019 11<span class="token punctuation">:</span>04<span class="token punctuation">:</span>03 +0200    <span class="token key atrule">Finished</span><span class="token punctuation">:</span>     Thu<span class="token punctuation">,</span> 10 Oct 2019 11<span class="token punctuation">:</span>14<span class="token punctuation">:</span>11 +0200</code></pre><p>退出代码 137 意味着该进程使用的内存超过允许的数量，必须被 OS 终止。</p><p>这是 Linux 中的一项功能，内核为系统中运行的进程设置 oom_score 值。此外，它还允许设置一个名为 oom_score_adj 的值，Kubernetes 使用该值来实现服务质量。它还具有 OOM Killer，它将检查进程并终止那些使用超过过多内存（比如申请了超过 limits 限制的数量的内存）的进程。</p><p>请注意，在 Kubernetes 中，进程可能会达到以下任何限制：</p><ul><li><p>在容器上设置的 Kubernetes 限制。</p></li><li><p>在 namespace 上设置的 Kubernetes ResourceQuota。</p></li><li><p>节点的实际内存大小。</p></li></ul><p><img src="/2024/06/20/kubernetes-zhong-cpu-he-nei-cun-shi-yong-xian-zhi/image.png" alt="Alt text"></p><h3 id="内存过量分配（overcommitment）"><a href="#内存过量分配（overcommitment）" class="headerlink" title="内存过量分配（overcommitment）"></a>内存过量分配（overcommitment）</h3><p>限制（limits）可以高于请求（requests），因此所有限制的总和可以高于节点容量。这称为过量分配，而且很常见。实际上，如果所有容器使用的内存多于 request 的内存，则可能会耗尽节点中的内存。这通常会导致一些 pod 死亡，以释放一些内存。</p><h3 id="监控-Kubernetes-OOM"><a href="#监控-Kubernetes-OOM" class="headerlink" title="监控 Kubernetes OOM"></a>监控 Kubernetes OOM</h3><p>在 Prometheus 生态中，使用 node-exporter 时，有一个名为 node_vmstat_oom_kill 的指标。跟踪 OOM 终止何时发生非常重要，但您可能希望在此类事件发生之前抢占先机并了解其情况。</p><p>我们更希望的是，检查进程与 Kubernetes limits 的接近程度：</p><pre><code>(sum by (namespace,pod,container)(rate(container_cpu_usage_seconds_total{container!=""}[5m])) / sum by (namespace,pod,container)(kube_pod_container_resource_limits{resource="cpu"})) &gt; 0.8</code></pre><h2 id="Kubernetes-CPU-throttling"><a href="#Kubernetes-CPU-throttling" class="headerlink" title="Kubernetes CPU throttling"></a>Kubernetes CPU throttling</h2><p>CPU 限制（throttling）是一种当进程即将达到某些资源限制时减慢速度的行为。与内存情况类似，这些限制可能是：</p><ul><li><p>在容器上设置的 Kubernetes limits。</p></li><li><p>在命名空间上设置的 Kubernetes ResourceQuota。</p></li><li><p>节点的实际算力大小。</p></li></ul><p>想想下面的类比。我们有一条高速公路，交通流量如下：</p><ul><li><p>CPU 就好比一条路</p></li><li><p>车辆代表 Process，每辆车都有不同的尺寸</p></li><li><p>多个通道代表有多个 CPU 核心</p></li><li><p>request 将是一条专用道路，例如自行车道</p></li></ul><p>这里的 throttling 被表示为交通拥堵：最终，所有进程都会运行，但一切都会变慢。</p><h3 id="Kubernetes-中的-CPU-处理逻辑"><a href="#Kubernetes-中的-CPU-处理逻辑" class="headerlink" title="Kubernetes 中的 CPU 处理逻辑"></a>Kubernetes 中的 CPU 处理逻辑</h3><p>CPU 在 Kubernetes 中通过 shares 进行处理。每个 CPU 核心被分为 1024 个 shares，然后使用 Linux 内核的 cgroups（control groups）功能在运行的所有进程之间进行划分。</p><p><img src="/2024/06/20/kubernetes-zhong-cpu-he-nei-cun-shi-yong-xian-zhi/image-1.png" alt="Alt text"></p><p>如果 CPU 可以处理当前所有进程，则无需执行任何操作。如果进程使用超过 100% 的 CPU，shares 机制就要起作用了。与任何 Linux 内核一样，Kubernetes 使用 CFS（Completely Fair Scheduler）机制，因此拥有更多份额的进程将获得更多的 CPU 时间。</p><p>与内存不同，Kubernetes 不会因为限流而杀死 Pod。</p><p><img src="/2024/06/20/kubernetes-zhong-cpu-he-nei-cun-shi-yong-xian-zhi/image-2.png" alt="Alt text"></p><h3 id="CPU-过渡分配"><a href="#CPU-过渡分配" class="headerlink" title="CPU 过渡分配"></a>CPU 过渡分配</h3><p>正如我们在limits 和 requests 文章中看到的，当我们想要限制进程的资源消耗时，设置 limits 或 requests 非常重要。尽管如此，请注意不要将总 requests 设置为大于实际 CPU 大小，每个容器都应该有保证的 CPU。</p><h3 id="监控-Kubernetes-CPU-throttling"><a href="#监控-Kubernetes-CPU-throttling" class="headerlink" title="监控 Kubernetes CPU throttling"></a>监控 Kubernetes CPU throttling</h3><p>您可以检查进程与 Kubernetes limits 的接近程度：</p><pre><code>(sum by (namespace,pod,container)(rate(container_cpu_usage_seconds_total{container!=""}[5m])) / sum by (namespace,pod,container)(kube_pod_container_resource_limits{resource="cpu"}))</code></pre><p>如果我们想要跟踪集群中发生的限制量，cadvisor 提供了 container_cpu_cfs_throttled_periods_total 和 container_cpu_cfs_periods_total 两个指标。通过这两个指标，您可以轻松计算所有 CPU 周期内的限制百分比。</p><h2 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h2><h3 id="注意-limits-和-requests"><a href="#注意-limits-和-requests" class="headerlink" title="注意 limits 和 requests"></a>注意 limits 和 requests</h3><p>Limits 是在节点中设置资源最大上限的一种方法，但需要谨慎对待，因为您可能最终会受到限制或终止进程。</p><h3 id="准备好应对驱逐"><a href="#准备好应对驱逐" class="headerlink" title="准备好应对驱逐"></a>准备好应对驱逐</h3><p>通过设置非常低的请求，您可能认为这将为您的进程授予最少的 CPU 或内存。但 kubelet 会首先驱逐那些使用率高于请求的 Pod，因此就相当于您将这些进程标记为最先被杀死的！</p><p>如果您需要保护特定 Pod 免遭抢占（当 kube-scheduler 需要分配新 Pod 时），请为最重要的进程分配 Priority Classes。</p><h3 id="Throttling-是一个无声的敌人"><a href="#Throttling-是一个无声的敌人" class="headerlink" title="Throttling 是一个无声的敌人"></a>Throttling 是一个无声的敌人</h3><p>设置不切实际的 limits 或过度使用，您可能没有意识到您的进程正在受到限制并且性能受到影响。主动监控 CPU 用量，了解确切的容器和命名空间层面的限制，及时发现问题非常重要。</p><h1 id="附"><a href="#附" class="headerlink" title="附"></a>附</h1><p>下面这张图，比较好的解释了 Kubernetes 中 CPU 和内存的限制问题。供参考：</p><p><img src="/2024/06/20/kubernetes-zhong-cpu-he-nei-cun-shi-yong-xian-zhi/image-3.png" alt="Alt text"></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> 可观测性 </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prometheus监控平台组件深度讲解</title>
      <link href="/2024/05/10/prometheus-jian-kong-ping-tai-zu-jian-shen-du-jiang-jie/"/>
      <url>/2024/05/10/prometheus-jian-kong-ping-tai-zu-jian-shen-du-jiang-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="Prometheus监控平台组件深度讲解"><a href="#Prometheus监控平台组件深度讲解" class="headerlink" title="Prometheus监控平台组件深度讲解"></a>Prometheus监控平台组件深度讲解</h1><p>Prometheus 的重要性和流行度已经无需多言。直入主题，本文对 Prometheus 监控平台的各个组件做深度讲解，希望能帮助读者更好地理解 Prometheus。</p><h2 id="监控系统的核心逻辑"><a href="#监控系统的核心逻辑" class="headerlink" title="监控系统的核心逻辑"></a>监控系统的核心逻辑</h2><p>对于一套监控系统而言，核心就是采集数据并存储，然后做告警判定、数据展示分析，这个 <a href="https://time.geekbang.org/column/article/622150">专栏文章</a> 详细讲解了这个数据流架构，整个流程图如下：</p><p><img src="/2024/05/10/prometheus-jian-kong-ping-tai-zu-jian-shen-du-jiang-jie/image.png" alt="监控系统的核心逻辑"></p><p>Prometheus 有多个组件（或者说多个进程），协同工作。下面我们逐个组件做一概述：</p><ul><li><a href="https://github.com/prometheus/prometheus%EF%BC%9A%E8%BF%99%E6%98%AF">https://github.com/prometheus/prometheus：这是</a> prometheus 进程的代码仓库，功能包括抓取远端监控指标、存储时序数据、暴露查询接口支持数据查询、支持告警规则配置并做告警判定</li><li><a href="https://github.com/prometheus/alertmanager%EF%BC%9A%E8%BF%99%E6%98%AF">https://github.com/prometheus/alertmanager：这是</a> alertmanager 进程的代码仓库，功能包括接收 prometheus 产生的告警事件，对事件做去重、分组、路由、通知等操作<br>我把监控系统的流程图给变换一下颜色：</li></ul><p><img src="/2024/05/10/prometheus-jian-kong-ping-tai-zu-jian-shen-du-jiang-jie/image-1.png" alt="监控系统核心逻辑"></p><ul><li>prometheus 进程承接了图中蓝色功能，即：采集器、时序库、告警判定引擎</li><li>alertmanager 进程负责告警事件分发，即图中红色部分</li><li>数据展示分析，橙色部分，Prometheus 做的比较少，Prometheus 确实有一个简单的 Web UI，不过比较简陋，一般使用更为强大的 Grafana 来做数据展示分析</li></ul><p>大家可能还听过各类 Exporter，难道这些 Exporter 就没有一席之地了么？Exporter 也是很重要的，可以看做是一个适配器，把监控目标的指标暴露出来，让 Prometheus 来抓取。或者把 Exporter 看做是采集器的一部分也行，无伤大雅，理解整个数据流就可以，无需在词汇上纠结。</p><p>想象一下，假设你有一个 Application，一个 Go 程序或者 Java Spring Boot 程序，Application 把自身的运行状态指标通过 /metrics 接口暴露出来，Prometheus 直接抓取即可，这里不需要什么 Exporter。但是一些成熟的数据库、中间件，比如 MySQL，Redis，这些软件没有直接暴露 Prometheus 格式的指标，Prometheus 没法直接来抓取，怎么办呢？当然，可以完善 Prometheus 的抓取器，让他不仅可以抓取 HTTP 协议的 /metrics 数据，也可以抓取 MySQL、Redis 等的数据，但是这样的话，Prometheus 代码会变得臃肿，不利于维护。所以，Prometheus 采用了 Exporter 的设计，Exporter 就是一个适配器，使用 Exporter 去抓取这些监控目标的指标，然后暴露为 Prometheus 格式的指标，Prometheus 再去抓取这些 Exporter 暴露的指标。这样做的好处是，Prometheus 代码保持简洁，Exporter 代码可以独立维护，提升整体可维护性。而且 Exporter 可以发动全网力量，让大家共建，一举多得。</p><p>但是，Exporter 会有很多不同的进程，水平参差不齐，从部署的角度可能略麻烦，所以市面上也有一些开源项目，把众多 Exporter 整合在一起变成一个进程，比如 Grafana-agent、Cprobe，当然，还有大名鼎鼎的 OpenTelemetry 也是这个思路。</p><p>了解了上述知识，我们再来看 Prometheus 官网的架构图。</p><h2 id="Prometheus-架构"><a href="#Prometheus-架构" class="headerlink" title="Prometheus 架构"></a>Prometheus 架构</h2><p><img src="/2024/05/10/prometheus-jian-kong-ping-tai-zu-jian-shen-du-jiang-jie/image-2.png" alt="Prometheus 架构"></p><ul><li>Prometheus Server：是 prometheus 进程的一部分功能，负责数据的抓取、存储、HTTP 接口查询</li><li>Retrieval：数据抓取，从监控目标那里拉取监控指标，Prometheus 定义了一个标准协议，只要监控目标支持这个协议，Prometheus 就可以抓取</li><li>TSDB：时序库，Prometheus 会把抓取到的监控指标存储在本地，单点的。如果想要高可用，可以使用 Thanos、VictoriaMetrics 等</li><li>HTTP server：Prometheus 会暴露 HTTP 接口，供外部查询监控指标</li><li>Service Discovery：服务发现，是 prometheus 进程的一部分功能，Prometheus 会定期去服务发现组件那里拉取监控目标的列表，省去了手动配置的繁琐，当然，前提是这些监控目标得注册到服务发现组件上<ul><li>Kubernetes SD：基于 Kubernetes 的服务发现机制，比如通过 apiserver 拉取 pod 列表、service 列表作为监控目标</li><li>File SD：基于文件的服务发现机制，从配置文件中读取监控目标列表</li><li>HTTP SD：基于 HTTP 的服务发现机制，从 HTTP 接口中读取监控目标列表</li><li>Consul SD：基于 Consul 的服务发现机制，从 Consul 中读取监控目标列表</li><li>等等</li></ul></li><li>Pushgateway：是一个单独的进程，用于接收短生命周期的监控指标，比如批处理任务的监控指标，因为批处理任务通常不会暴露 HTTP 接口，Prometheus 就没法拉取了，所以批处理任务需要主动推送监控指标到 Pushgateway，Prometheus 再去拉取 Pushgateway 的监控指标</li><li>Alertmanager：负责接收 prometheus 产生的告警事件，对事件做去重、分组、路由、通知等操作。如果想要更高阶的收敛、降噪、排班、认领、升级等功能，可以把 Alertmanager 和一些第三方工具结合使用，比如 PagerDuty、FlashDuty、OpsGenie 等</li><li>Prometheus web UI：prometheus 进程启动之后，会暴露一个简单的 Web UI，可以查看监控指标，但是功能比较简陋，一般使用Grafana 来做数据展示分析</li><li>Grafana：是一个独立的进程，不属于 Prometheus 项目的一部分，不过可以和 Prometheus 整合。用于数据展示分析，功能非常强大，支持多种数据源，比如 Prometheus、Elasticsearch、Loki 等，支持多种图表类型，比如折线图、柱状图、饼图、热力图等</li></ul><h2 id="Prometheus-架构的问题"><a href="#Prometheus-架构的问题" class="headerlink" title="Prometheus 架构的问题"></a>Prometheus 架构的问题</h2><p>主要问题的容量扩展问题。Prometheus 一个进程干了很多事情，部署非常简单，弊端就是单点没法扩展，比如告警引擎是单点、存储是单点、采集是单点，如果体量很大或者对稳定性要求比较高，就需要通过其他手段来解决了。</p><p>比如 VictoriaMetrics 项目，就是完全兼容 Prometheus 生态的协议和接口，但是提供了分布式能力。存储使用 vmstorage 进程，查询使用 vmselect 进程，数据接收使用 vminsert，告警使用 vmalert，数据抓取使用 vmagent，组件确实多了，但是每个组件都可以部署多个实例组成集群，提升了整体的可用性和容量。VictoriaMetrics 项目的架构图如下：</p><p><img src="/2024/05/10/prometheus-jian-kong-ping-tai-zu-jian-shen-du-jiang-jie/image-3.png" alt="Alt text"></p><p>或者还有一个办法，就是直接部署多套 Prometheus，比如 DBA 自己用一个 Prometheus，Hadoop 团队自己用一个 Prometheus，这样可以解决容量问题，没法解决数据单点存储问题。如何解决单点问题？双写！比如 DBA 团队，部署两个 Prometheus，采集相同的数据，两个 Prometheus 数据相同，规则相同，告警也会产生两份，可以通过 Alertmanager 做告警去重，这样就解决了单点问题。</p><h2 id="Prometheus-规则管理问题"><a href="#Prometheus-规则管理问题" class="headerlink" title="Prometheus 规则管理问题"></a>Prometheus 规则管理问题</h2><p>最后一个问题，简单聊聊 Prometheus 的规则管理问题。Prometheus 的规则是通过配置文件定义的，这个配置文件是一个 yaml 文件，里面定义了监控规则、告警规则等。如果一个公司有很多套 Prometheus，规则分散在多个 yaml 中不方便管理，希望能有一套易用的、权限隔离的 UI，把监控能力开放给全公司各个团队并让他们自服务，别啥事都来找监控团队，这个时候就需要一个规则管理系统，比如夜莺（Nightingale）。如果有这方面的痛点可以去了解一下，如果 Prometheus 自身的玩法就感觉够用了，那更好，不用再引入新的组件。</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 可观测性 </tag>
            
            <tag> 监控 </tag>
            
            <tag> Prometheus </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CloudFlare代理GithubPage301问题</title>
      <link href="/2024/04/30/cloudflare-dai-li-githubpage301-wen-ti/"/>
      <url>/2024/04/30/cloudflare-dai-li-githubpage301-wen-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="CloudFlare代理GithubPage301问题"><a href="#CloudFlare代理GithubPage301问题" class="headerlink" title="CloudFlare代理GithubPage301问题"></a>CloudFlare代理GithubPage301问题</h1><p>由于GithubPage是美国服务器，国内访问速度较慢，所以使用CloudFlare加速。</p><p>注册cloudflare后，设置DNS解析，并开启proxy。</p><p>此时打开网站发现网站被循环301跳转无法正常访问<br><img src="/2024/04/30/cloudflare-dai-li-githubpage301-wen-ti/image.png" alt="Alt text"></p><p>检查cloudflare的配置，发现SSL配置使用的是flexible模式，这种模式回源是通过http协议，而githubpage又配置了强制https跳转，所以导致循环301跳转。</p><p><img src="/2024/04/30/cloudflare-dai-li-githubpage301-wen-ti/image-1.png" alt="Alt text"></p><p>网站正常访问</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CloudFlare </tag>
            
            <tag> GithubPages </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB集群-带访问控制的分片副本集</title>
      <link href="/2024/04/25/mongodb-ji-qun-dai-fang-wen-kong-zhi-de-fen-pian-fu-ben-ji/"/>
      <url>/2024/04/25/mongodb-ji-qun-dai-fang-wen-kong-zhi-de-fen-pian-fu-ben-ji/</url>
      
        <content type="html"><![CDATA[<h1 id="MongoDB集群-带访问控制的分片副本集"><a href="#MongoDB集群-带访问控制的分片副本集" class="headerlink" title="MongoDB集群-带访问控制的分片副本集"></a>MongoDB集群-带访问控制的分片副本集</h1><h2 id="1-环境："><a href="#1-环境：" class="headerlink" title="1.环境："></a>1.环境：</h2><p>集群部署采用三台服务器，部署三个分片，每个分片三个副本。各个分片之间是完全独立的，一个database的数据只能落在一个分片上。</p><pre><code>server：172.18.1.31 172.18.1.32 172.18.1.33OS：CentOS 7.2MongoDB Version：v4.0.5</code></pre><p>服务部署结构如下：</p><table><thead><tr><th>172.18.1.31</th><th>172.18.1.32</th><th>172.18.1.33</th></tr></thead><tbody><tr><td>config:27017</td><td>config:27017</td><td>config:27017</td></tr><tr><td>mongos:27018</td><td>mongos:27018</td><td>mongos:27018</td></tr><tr><td>shard01:27101</td><td>shard01:27101</td><td>shard01:27101</td></tr><tr><td>shard02:27102</td><td>shard02:27102</td><td>shard02:27102</td></tr><tr><td>shard03:27103</td><td>shard03:27103</td><td>shard03:27103</td></tr></tbody></table><h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2.安装"></a>2.安装</h2><h3 id="2-1-添加yum源"><a href="#2-1-添加yum源" class="headerlink" title="2.1 添加yum源"></a>2.1 添加yum源</h3><pre class=" language-shell"><code class="language-shell">    cat > /etc/yum.repos.d/mongodb-org-4.0.repo << EOF    [mongodb-org-4.0]    name=MongoDB Repository    baseurl=https://repo.mongodb.org/yum/redhat/7/mongodb-org/4.0/x86_64/    gpgcheck=1    enabled=1    gpgkey=https://www.mongodb.org/static/pgp/server-4.0.asc    EOF</code></pre><h3 id="2-2-安装"><a href="#2-2-安装" class="headerlink" title="2.2 安装"></a>2.2 安装</h3><pre class=" language-shell"><code class="language-shell">    # yum -y install mongodb-org</code></pre><h3 id="2-3-删除yum安装的mongod服务"><a href="#2-3-删除yum安装的mongod服务" class="headerlink" title="2.3 删除yum安装的mongod服务"></a>2.3 删除yum安装的mongod服务</h3><pre class=" language-shell"><code class="language-shell">    # systemctl disable mongod    # rm –f /usr/lib/systemd/system/mongod.service    # systemctl daemon-reload</code></pre><h2 id="3-高可用集群部署"><a href="#3-高可用集群部署" class="headerlink" title="3.高可用集群部署"></a>3.高可用集群部署</h2><h3 id="3-1-配置文件准备"><a href="#3-1-配置文件准备" class="headerlink" title="3.1 配置文件准备"></a>3.1 配置文件准备</h3><p>每台服务器上都运行monogs、config、shard01、shard02、shard03服务，分别对应一个配置文件，统一将配置文件存放在/etc/mongodb/目录下。</p><pre class=" language-shell"><code class="language-shell">    # mkdir /etc/mongodb    # chown -R mongod.mongod /etc/mongodb</code></pre><p>将config和shard的数据保存在/data/mongodb/目录下。</p><pre class=" language-shell"><code class="language-shell">    # mkdir -p /data/mongodb/{config,shard01,shard02,shard03}/data /data/mongodb/mongos    # chown -R mongod.mongod /data/mongodb</code></pre><p>日志统一放在/data/logs/mongodb目录下</p><pre class=" language-shell"><code class="language-shell">    # mkdir /data/logs/mongodb    # chown -R mongod.mongod /data/logs/mongodb</code></pre><p>config配置</p><pre class=" language-shell"><code class="language-shell">    vim /etc/mongodb/config.conf    # where to write logging data.    systemLog:      destination: file      logAppend: true      path: /data/logs/mongodb/config.log    # Where and how to store data.    storage:      dbPath: /data/mongodb/config/data      journal:        enabled: true    # how the process runs    processManagement:      fork: true      pidFilePath: /data/mongodb/config/mongodb-config.pid      timeZoneInfo: /usr/share/zoneinfo    # network interfaces    net:      port: 27018      bindIp: 0.0.0.0      unixDomainSocket:        pathPrefix: /var/run/mongodb    #operationProfiling:    replication:        replSetName: ussmongo-config    sharding:        clusterRole: configsvr</code></pre><p>shard01配置</p><pre class=" language-shell"><code class="language-shell">    cat /etc/mongodb/shard01.conf     # where to write logging data.    systemLog:      destination: file      logAppend: true      path: /data/logs/mongodb/shard01.log      logRotate: rename    # Where and how to store data.    storage:      dbPath: /data/mongodb/shard01/data      journal:        enabled: true      wiredTiger:        engineConfig:           cacheSizeGB: 20    # how the process runs    processManagement:      fork: true      pidFilePath: /data/mongodb/shard01/mongodb-shard01.pid      timeZoneInfo: /usr/share/zoneinfo    # network interfaces    net:      port: 27101      bindIp: 0.0.0.0      unixDomainSocket:        pathPrefix: /var/run/mongodb    #operationProfiling:    replication:        replSetName: ussmongo-shard01    sharding:        clusterRole: shardsvr</code></pre><p>shard02配置</p><pre class=" language-shell"><code class="language-shell">    cat /etc/mongodb/shard02.conf     # where to write logging data.    systemLog:      destination: file      logAppend: true      path: /data/logs/mongodb/shard02.log    # Where and how to store data.    storage:      dbPath: /data/mongodb/shard02/data      journal:        enabled: true      wiredTiger:        engineConfig:           cacheSizeGB: 20    # how the process runs    processManagement:      fork: true      pidFilePath: /data/mongodb/shard02/mongodb-shard02.pid      timeZoneInfo: /usr/share/zoneinfo    # network interfaces    net:      port: 27102      bindIp: 0.0.0.0      unixDomainSocket:        pathPrefix: /var/run/mongodb    #operationProfiling:    replication:        replSetName: ussmongo-shard02    sharding:        clusterRole: shardsvr</code></pre><p>shard03配置</p><pre class=" language-shell"><code class="language-shell">    # cat /etc/mongodb/shard03.conf     # where to write logging data.    systemLog:      destination: file      logAppend: true      path: /data/logs/mongodb/shard03.log    # Where and how to store data.    storage:      dbPath: /data/mongodb/shard03/data      journal:        enabled: true      wiredTiger:        engineConfig:           cacheSizeGB: 20    # how the process runs    processManagement:      fork: true      pidFilePath: /data/mongodb/shard03/mongodb-shard03.pid      timeZoneInfo: /usr/share/zoneinfo    # network interfaces    net:      port: 27103      bindIp: 0.0.0.0      unixDomainSocket:        pathPrefix: /var/run/mongodb    #operationProfiling:    replication:        replSetName: ussmongo-shard03    sharding:        clusterRole: shardsvr</code></pre><p>mongos配置</p><pre class=" language-shell"><code class="language-shell">    # cat /etc/mongodb/mongos.conf     systemLog:      destination: file      logAppend: true      path: /data/logs/mongodb/mongos.log    processManagement:      fork: true    #  pidFilePath: /data/mongodb/mongos.pid    # network interfaces    net:      port: 27017      bindIp: 0.0.0.0      unixDomainSocket:        pathPrefix: /var/run/mongodb    sharding:       configDB: ussmongo-config/172.18.1.31:27018,172.18.1.32:27018,172.18.1.33:27018    setParameter:      diagnosticDataCollectionDirectoryPath: /data/mongodb/mongos/diagnostic.data/</code></pre><h3 id="3-2-服务文件准备"><a href="#3-2-服务文件准备" class="headerlink" title="3.2 服务文件准备"></a>3.2 服务文件准备</h3><p>为了方便对进程的统一管理，将其以服务的形式运行，亦可以使其开机自动启动。</p><p>mongo-shard</p><pre class=" language-shell"><code class="language-shell">    # cat /usr/lib/systemd/system/mongo-shard@.service     [Unit]    Description=MongoDB Database Shard Service    After=network.target    Documentation=https://docs.mongodb.org/manual    PartOf=mongo-shard.target    [Service]    User=mongod    Group=mongod    Environment="OPTIONS=--quiet -f /etc/mongodb/shard%i.conf"    EnvironmentFile=-/etc/sysconfig/mongod    ExecStart=/usr/bin/mongod $OPTIONS    PermissionsStartOnly=true    Type=forking    TasksMax=infinity    TasksAccounting=false    [Install]    WantedBy=mongo-shard.target</code></pre><p>mongo-config</p><pre class=" language-shell"><code class="language-shell">    # cat /usr/lib/systemd/system/mongo-config.service     [Unit]    Description=MongoDB Database Config Service    After=network.target    Documentation=https://docs.mongodb.org/manual    PartOf=mongo.target    [Service]    User=mongod    Group=mongod    Environment="OPTIONS=--quiet -f /etc/mongodb/config.conf"    EnvironmentFile=-/etc/sysconfig/mongod    ExecStart=/usr/bin/mongod $OPTIONS    PermissionsStartOnly=true    Type=forking    TasksMax=infinity    TasksAccounting=false    [Install]    WantedBy=mongo.target</code></pre><p>mongo-mongos</p><pre class=" language-shell"><code class="language-shell">    # cat /usr/lib/systemd/system/mongos.service     [Unit]    Description=MongoDB Database Service    After=syslog.target network.target    PartOf=mongo.target    [Service]    User=mongod    Group=mongod    Environment="OPTIONS=--quiet -f /etc/mongodb/mongos.conf"    ExecStart=/usr/bin/mongos $OPTIONS    Type=forking    PrivateTmp=true    LimitNOFILE=64000    TimeoutStartSec=180    [Install]    WantedBy=mongo.target    </code></pre><p>为了便于批量管理，创建target文件</p><pre class=" language-shell"><code class="language-shell">/usr/lib/systemd/system/mongo-shard.target    [Unit]    Description=mongo shard target allowing to start/stop all mongo-shard@.service instances at once    PartOf=mongo.target    [Install]    WantedBy=mongo.target/usr/lib/systemd/system/mongo.target    [Unit]    Description=mongo target allowing to start/stop all mongo*.service instances at once    [Install]    WantedBy=multi-user.target</code></pre><p>加载服务并使其开机自启动：</p><pre class=" language-shell"><code class="language-shell">    # systemctl daemon-reload    # systemctl enable mongo-shard@01    # systemctl enable mongo-shard@02    # systemctl enable mongo-shard@03    # systemctl enable mongo-config    # systemctl enable mongos    # systemctl enable mongo-shard.target    # systemctl enable mongo.target</code></pre><p>将上述配置文件及服务文件copy到所有节点的对于目录下</p><p>启动服务</p><pre><code>    systemctl start mongo.target</code></pre><p>这个时候mongos会启动不了，需要先配置副本集</p><h3 id="3-3-配置副本集"><a href="#3-3-配置副本集" class="headerlink" title="3.3 配置副本集"></a>3.3 配置副本集</h3><p>config和shard服务本质上都是mongod进程，将他们都配置为三副本模式。下面的操作可以在三个节点中的任意一个上执行，只需要执行一遍。</p><p>config副本集</p><pre class=" language-shell"><code class="language-shell">    config = {    _id : "ussmongo-config",     members : [         {_id : 0, host : "172.18.1.31:27018" },         {_id : 1, host : "172.18.1.32:27018" },         {_id : 2, host : "172.18.1.33:27018" }     ]    }</code></pre><p>shard01副本集</p><pre class=" language-shell"><code class="language-shell">    config = {    _id : "ussmongo-shard01",     members : [         {_id : 0, host : "172.18.1.31:27101" },         {_id : 1, host : "172.18.1.32:27101" },         {_id : 2, host : "172.18.1.33:27101" }     ]    }</code></pre><p>shard02副本集</p><pre class=" language-shell"><code class="language-shell">    config = {    _id : "ussmongo-shard02",     members : [         {_id : 0, host : "172.18.1.31:27102" },         {_id : 1, host : "172.18.1.32:27102" },         {_id : 2, host : "172.18.1.33:27102" }     ]    }</code></pre><p>shard03副本集</p><pre class=" language-shell"><code class="language-shell">    config = {    _id : "ussmongo-shard03",     members : [         {_id : 0, host : "172.18.1.31:27103" },         {_id : 1, host : "172.18.1.32:27103" },         {_id : 2, host : "172.18.1.33:27103" }     ]    }</code></pre><p>此时，重启集群，mongs服务可以正常启动</p><pre class=" language-shell"><code class="language-shell">    systemctl start mongo.target</code></pre><h3 id="3-4-配置路由分片"><a href="#3-4-配置路由分片" class="headerlink" title="3.4 配置路由分片"></a>3.4 配置路由分片</h3><p>mongos对外提供服务，是集群的入口。需要先将分片添加到mongos配置中：<br>需要在所有节点执行</p><pre class=" language-shell"><code class="language-shell">    # mongo --port 27017    mongos> use admin    mongos> sh.addShard("ussmongo-shard01/172.18.1.31:27101,172.18.1.32:27101,172.18.1.33:27101")    mongos> sh.addShard("ussmongo-shard02/172.18.1.31:27102,172.18.1.32:27102,172.18.1.33:27102")    mongos> sh.addShard("ussmongo-shard03/172.18.1.31:27103,172.18.1.32:27103,172.18.1.33:27103")</code></pre><p>至此，多副本分片的高可用集群搭建完成，mongodb可以正常对外提供服务，可以在三台mongos之上再配置上负载均衡，高可用集群就完成了。</p><h2 id="4-启动访问控制"><a href="#4-启动访问控制" class="headerlink" title="4.启动访问控制"></a>4.启动访问控制</h2><p>在生产环境中，没有认证连接集群是不安全的，也是不允许的，所以都需要开启安全认证。</p><h3 id="4-1-添加用户"><a href="#4-1-添加用户" class="headerlink" title="4.1 添加用户"></a>4.1 添加用户</h3><p>连接上mongos添加的用户会保存在config副本集中，但是不会保存在shard副本集中，因此添加用户的操作需要分别在config、shard01、shard02、shard03上执行。<br>在执行的时候，只有在primary节点上才能执行成功，注意执行节点。</p><p>config副本集：</p><pre class=" language-shell"><code class="language-shell">    # mongo --port 27018    > use admin    > db.createUser(       {         user: "admin",         pwd: "admin",         roles: ["userAdminAnyDatabase", "dbAdminAnyDatabase", "readWriteAnyDatabase", "clusterAdmin"]       }     )</code></pre><p>shard01副本集</p><pre class=" language-shell"><code class="language-shell">    # mongo --port 27101    > use admin    > db.createUser(       {         user: "admin",         pwd: "admin",         roles: ["userAdminAnyDatabase", "dbAdminAnyDatabase", "readWriteAnyDatabase", "clusterAdmin"]       }    )</code></pre><p>shard02副本集</p><pre class=" language-shell"><code class="language-shell">    # mongo --port 27102    > use admin    > db.createUser(       {         user: "admin",         pwd: "admin",         roles: ["userAdminAnyDatabase", "dbAdminAnyDatabase", "readWriteAnyDatabase", "clusterAdmin"]       }    )    </code></pre><p>shard03副本集</p><pre class=" language-shell"><code class="language-shell">mongo --port 27103    > use admin    > db.createUser(       {         user: "admin",         pwd: "admin",         roles: ["userAdminAnyDatabase", "dbAdminAnyDatabase", "readWriteAnyDatabase", "clusterAdmin"]       }    )</code></pre><h3 id="4-2启用访问控制"><a href="#4-2启用访问控制" class="headerlink" title="4.2启用访问控制"></a>4.2启用访问控制</h3><p>创建密钥文件<br>启用访问控制之后，外部访问MongoDB服务需要进行身份验证，而mongos访问config和shard服务则是通过配置的秘钥文件。</p><pre class=" language-shell"><code class="language-shell">openssl rand -base64 756 >/data/mongodb/ussmongo.keychmod 0600 /data/mongodb/ussmongo.keychown mongod:mongod /data/mongodb/ussmongo.key</code></pre><p>将密钥文件复制到所有的节点上</p><p>添加security配置<br>mongos配置增加</p><pre class=" language-yaml"><code class="language-yaml">    <span class="token key atrule">security</span><span class="token punctuation">:</span>      <span class="token key atrule">keyFile</span><span class="token punctuation">:</span> /data/mongodb/ussmongo.key</code></pre><p>config和shard配置增加</p><pre class=" language-yaml"><code class="language-yaml">    <span class="token key atrule">security</span><span class="token punctuation">:</span>      <span class="token key atrule">authorization</span><span class="token punctuation">:</span> enabled      <span class="token key atrule">keyFile</span><span class="token punctuation">:</span> /data/mongodb/ussmongo.key</code></pre><p>重启所有服务</p><pre class=" language-shell"><code class="language-shell">    # systemctl restart mongo.target</code></pre><p>至此带访问控制的Mongodb高可用集群部署完成</p><h2 id="5-日志切割"><a href="#5-日志切割" class="headerlink" title="5.日志切割"></a>5.日志切割</h2><h3 id="5-1-创建日志切割脚本"><a href="#5-1-创建日志切割脚本" class="headerlink" title="5.1 创建日志切割脚本"></a>5.1 创建日志切割脚本</h3><pre class=" language-shell"><code class="language-shell">    cat /opt/scripts/lograte_mongod.sh     #!/bin/bash    #Rotate the MongoDB logs to prevent a single logfile from consuming too much disk space.    app=mongod    mongodPath=/usr/bin    pidArray=$(pidof $mongodPath/$app)    for pid in $pidArray;do    if [ $pid ]    then        echo $pid        kill -USR1 $pid    fi    done    exit        chmod +x /opt/scripts/lograte_mongod.sh</code></pre><h3 id="5-2-定时任务"><a href="#5-2-定时任务" class="headerlink" title="5.2 定时任务"></a>5.2 定时任务</h3><pre class=" language-shell"><code class="language-shell">    crontab -e    # mognod log lograte    00 00 * * * /opt/scripts/lograte_mongod.sh >/dev/null 2>&1</code></pre><p>参考文档：<a href="https://yq.aliyun.com/articles/625991?spm=5176.10695662.1996646101.searchclickresult.374f3653bPUSs8">https://yq.aliyun.com/articles/625991?spm=5176.10695662.1996646101.searchclickresult.374f3653bPUSs8</a></p><h2 id="6-问题处理"><a href="#6-问题处理" class="headerlink" title="6.问题处理"></a>6.问题处理</h2><h3 id="6-1-MongDB集群异常关闭修复"><a href="#6-1-MongDB集群异常关闭修复" class="headerlink" title="6.1 MongDB集群异常关闭修复"></a>6.1 MongDB集群异常关闭修复</h3><p>机器异常重启后，集群无法启动，需要修复数据</p><pre class=" language-shell"><code class="language-shell"># 删除lock文件rm -rf /data/mongodb/data/mongod.lockrm -rf /data/mongodb/data/WiredTiger.lock# 数据修复,每个分片都需要执行修复操作mongod --repair -f /etc/mongodb/shard01.conf -nojournal --repairpath /data/mongodb/shard01/data/diagnostic.data/mongod --repair -f /etc/mongodb/shard02.conf -nojournal --repairpath /data/mongodb/shard02/data/diagnostic.data/mongod --repair -f /etc/mongodb/shard03.conf -nojournal --repairpath /data/mongodb/shard03/data/diagnostic.data/# 重启分片systemctl restart mongo-shard@01systemctl restart mongo-shard@02systemctl restart mongo-shard@03systemctl restart mongo-configsystemctl restart mongos</code></pre>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MongoDB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes-v1.30新特性一览</title>
      <link href="/2024/04/16/kubernetes-v1-30-xin-te-xing-yi-lan/"/>
      <url>/2024/04/16/kubernetes-v1-30-xin-te-xing-yi-lan/</url>
      
        <content type="html"><![CDATA[<h2 id="Kubernetes-v1-30-新特性一览"><a href="#Kubernetes-v1-30-新特性一览" class="headerlink" title="Kubernetes v1.30 新特性一览"></a>Kubernetes v1.30 新特性一览</h2><p>Kubernetes v1.30 是 2024 年发布的第一个大版本，包含了 45 项主要的更新。 对比去年的话，v1.27 有近 60 项，v1.28 有 46 项，v1.29 有 49 项。可以看到 Kubernetes 变得更加谨慎了，会更加保守的控制进入其核心的功能。</p><p>恰好前些天我在 “硬地骇客” 播客上录制了一期节目，正好提到 Kubernetes 现在是不是变得太复杂了，感兴趣的话可以听一下。</p><p>具体来看 v1.30 版本中有 10 个增强功能正在进入 Alpha 阶段，18 个将升级到 Beta 阶段，而另外 17 个则将升级到稳定版。</p><p>这次的版本称之为 “Uwernetes” 是 UwU 和 Kubernetes 的组合，因为发布团队认为这个版本是最可爱的版本了，也是对所有参与到 Kubernetes 生态中的人的一种致敬。让我们一起探索这个充满爱和喜悦的版本吧 UwU</p><p><img src="/2024/04/16/kubernetes-v1-30-xin-te-xing-yi-lan/image.png" alt="Alt text"></p><h3 id="Pod-调度就绪机制达到-GA"><a href="#Pod-调度就绪机制达到-GA" class="headerlink" title="Pod 调度就绪机制达到 GA"></a>Pod 调度就绪机制达到 GA</h3><p>这个功能实际上是从 Kubernetes v1.26 开始增加的。是 KEP 3521 的第一部分。并且在 v1.27 时候达到了 Beta。</p><p>我们来快速的回顾一下 Pod 的创建过程。</p><p>当 client 通过 kube-apiserver 创建成功 Pod 资源后，kube-scheduler 会去检查尚未被调度的 Pod，然后为其进行调度，分配 Node。之后 Node 获取到调度到该 Node 上的 Pod 然后进行创建。（这里省略了很多细节，但其他的部分与我们此处要介绍的内容关系不太大，就不展开了。）</p><p>根据上述的过程，我们可以发现，在 Pod 创建成功后，其实就默认该 Pod 是可以被调度了，kube-scheduler 就应该开始工作了。</p><p>但在实际的场景中，Pod 通常还会需要一些其他的资源，最典型的比如存储。在一些环境中，这些资源是需要预先进行创建的，尤其是在一些云厂商的场景中，还需要检查用户账户中是否还有余额可以用于创建云盘等。</p><p>一但前置的依赖无法满足，假如 kube-scheduler 已经完成了 Pod 的调度，那么 kubelet 侧就会处于尝试创建 Pod ，但失败的情况。</p><p>这个 KEP 的出现就可以很好的解决这个问题，增加了一个 Pod 是否准备好被调度的机制。如果前置依赖不满足，那么 Pod 就无需被调度，这也不会消耗资源。kube-scheduler 和 kubelet 都无需进行处理。待条件满足，Pod 再被调度和创建即可。</p><p><img src="/2024/04/16/kubernetes-v1-30-xin-te-xing-yi-lan/image-1.png" alt="Alt text"></p><p>这个机制我个人感觉还是挺好的，甚至我可以有更灵活的策略来控制应用的副本。比如在大流量，需要动态扩容的场景下，我可以利用此机制预先创建一些 Pod 资源以及准备好它的依赖， 甚至可以在 Node 上准备好镜像等。当需要增加副本时，直接标记 Pod 可被调度，</p><p>使用时，通过配置 Pod 的 .spec.schedulingGates 即可。例如：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>pod<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">schedulingGates</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> moelove.info/disk  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> foo.bar/xyz  <span class="token key atrule">containers</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> kong    <span class="token key atrule">image</span><span class="token punctuation">:</span> kong<span class="token punctuation">:</span><span class="token number">3.6</span></code></pre><p>注意这里可以为它设置多个 schedulingGates 但是要确保它们的唯一性。</p><p>如果将上述配置应用到 Kubernetes 集群中，则会看到</p><pre class=" language-shell"><code class="language-shell">➜  ~ kubectl get pods NAME       READY   STATUS            RESTARTS   AGEtest-pod   0/1     SchedulingGated   0          17s</code></pre><p>.status 中也会有相关输出：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">status</span><span class="token punctuation">:</span>  <span class="token key atrule">conditions</span><span class="token punctuation">:</span>  <span class="token punctuation">-</span> <span class="token key atrule">lastProbeTime</span><span class="token punctuation">:</span> <span class="token null important">null</span>    <span class="token key atrule">lastTransitionTime</span><span class="token punctuation">:</span> <span class="token null important">null</span>    <span class="token key atrule">message</span><span class="token punctuation">:</span> Scheduling is blocked due to non<span class="token punctuation">-</span>empty scheduling gates    <span class="token key atrule">reason</span><span class="token punctuation">:</span> SchedulingGated    <span class="token key atrule">status</span><span class="token punctuation">:</span> <span class="token string">"False"</span>    <span class="token key atrule">type</span><span class="token punctuation">:</span> PodScheduled  <span class="token key atrule">phase</span><span class="token punctuation">:</span> Pending  <span class="token key atrule">qosClass</span><span class="token punctuation">:</span> BestEffort</code></pre><p>那么何时能正常调度呢？我们在实现对应的 controller 或者 operator 时，只需要移除 schedulingGates 的配置即可。</p><h3 id="kubectl-交互式删除达到-GA"><a href="#kubectl-交互式删除达到-GA" class="headerlink" title="kubectl 交互式删除达到 GA"></a>kubectl 交互式删除达到 GA</h3><p>很多人都会直接使用 kubectl 进行 Kubernetes 集群的管理，包括资源的创建和删除等。删除操作是很危险的动作，并且不可逆，如果由于错误的拼写，不小心的复制粘贴或者错误的补全等， 不小心误删了重要的资源，可能会带来一些不小的麻烦。</p><p>这是一般情况下的删除操作，在回车后就直接执行了。</p><pre class=" language-shell"><code class="language-shell">k8s-test:~$ kubectl create secret generic my-secret --from-literal=key1=supersecretsecret/my-secret createdk8s-test:~$ kubectl delete secret my-secret secret "my-secret" deleted</code></pre><p>在 KEP-3895 中提出可以为 kubectl delete 增加一个 -i 的选项，类似于 Linux 中的 rm -i 命令那样，在得到用户确认后再进行删除操作，如果用户否认则退出删除操作。</p><p>我们来看下效果：</p><pre class=" language-shell"><code class="language-shell">k8s-test:~$ kubectl create secret generic my-secret2 --from-literal=key1=supersecret secret/my-secret2 createdk8s-test:~$ kubectl delete -i secret my-secret2You are about to delete the following 1 resource(s):secret/my-secret2Do you want to continue? (y/n): ndeletion is cancelledk8s-test:~$ kubectl delete -i secret my-secret2You are about to delete the following 1 resource(s):secret/my-secret2Do you want to continue? (y/n): ysecret "my-secret2" deleted</code></pre><p>这个特性是在 Kubernetes v1.27 作为 Alpha 特性引入的，在 Alpha 阶段，需要添加 KUBECTL_INTERACTIVE_DELETE=true 的环境变量进行开启；在 v1.29 达到了 beta 阶段，不再需要环境变量即可使用，如今在 v1.30 达到了 GA，建议大家可以加个 alias 会比较方便。</p><h3 id="基于-CEL-的-Adminssion-Control-达到-GA"><a href="#基于-CEL-的-Adminssion-Control-达到-GA" class="headerlink" title="基于 CEL 的 Adminssion Control 达到 GA"></a>基于 CEL 的 Adminssion Control 达到 GA</h3><p>对 Kubernetes 中的 Adminssion Control 感兴趣的小伙伴可以看看我之前的文章：理清 Kubernetes 中的准入控制（Admission Controller) | MoeLove</p><ul><li>KEP-3488: Implement secondary authz for ValidatingAdmissionPolicy by jpbetz · Pull Request #116054 · kubernetes/kubernetes</li><li>KEP-3488: Implement Enforcement Actions and Audit Annotations by jpbetz · Pull Request #115973 · kubernetes/kubernetes<br>Kubernetes 在 v1.26 增加了一项很重要的特性，那就是允许使用 CEL 进行 Admission Control，具体内容可以参考我之前写的文章：Kubernetes v1.26 新特性一览 | MoeLove</li></ul><p>其中引入了一个新的资源 ValidatingAdmissionPolicy ，使用起来如下:</p><h3 id="v1-26-使用的-API-Version-是-v1alpha1"><a href="#v1-26-使用的-API-Version-是-v1alpha1" class="headerlink" title="v1.26 使用的 API Version 是 v1alpha1"></a>v1.26 使用的 API Version 是 v1alpha1</h3><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> admissionregistration.k8s.io/v1alpha1 <span class="token key atrule">kind</span><span class="token punctuation">:</span> ValidatingAdmissionPolicy <span class="token key atrule">metadata</span><span class="token punctuation">:</span>   <span class="token key atrule">name</span><span class="token punctuation">:</span> <span class="token string">"demo-policy.moelove.info"</span> <span class="token key atrule">Spec</span><span class="token punctuation">:</span>   <span class="token key atrule">failurePolicy</span><span class="token punctuation">:</span> Fail   <span class="token key atrule">matchConstraints</span><span class="token punctuation">:</span>     <span class="token key atrule">resourceRules</span><span class="token punctuation">:</span>     <span class="token punctuation">-</span> <span class="token key atrule">apiGroups</span><span class="token punctuation">:</span>   <span class="token punctuation">[</span><span class="token string">"apps"</span><span class="token punctuation">]</span>       <span class="token key atrule">apiVersions</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"v1"</span><span class="token punctuation">]</span>       <span class="token key atrule">operations</span><span class="token punctuation">:</span>  <span class="token punctuation">[</span><span class="token string">"CREATE"</span><span class="token punctuation">,</span> <span class="token string">"UPDATE"</span><span class="token punctuation">]</span>       <span class="token key atrule">resources</span><span class="token punctuation">:</span>   <span class="token punctuation">[</span><span class="token string">"deployments"</span><span class="token punctuation">]</span>   <span class="token key atrule">validations</span><span class="token punctuation">:</span>     <span class="token punctuation">-</span> <span class="token key atrule">expression</span><span class="token punctuation">:</span> <span class="token string">"object.spec.replicas &lt;= 2"</span></code></pre><p>但是在当时也只是实现了 KEP-3488 的一部分。</p><p>比如实现 Authz 的部分，允许用 CEL 表达式编写复杂的 admission control 规则， 放置在声明式资源中，而不是构建和部署 webhook。</p><p>虽然 admission webhook 一直是我们灵活的与第三方工具集成的基石，但是对于新用户来说，它们有很多复杂性，新的 CEL 系统有望接管仅需要对默认规则进行小修改的简单独立案例。</p><p>以前，表达式上下文会公开有关当前请求和目标资源的信息，现在通过这个 #116054 PR 可以在授权层中动态检查 RBAC 权限。一些有用的地方可能是使用 RBAC 进行每个字段的更新权限，允许 RBAC 检查特定对象而不使用 resourceNames 系统， 或基于请求者身份限制对程序敏感字段（例如 finalizers ）的访问而无需生成复杂的 RBAC 策略。</p><p>例如：</p><pre class=" language-yaml"><code class="language-yaml">authorizer.group('').resource('pods').namespace('default').check('create').allowed()authorizer.path('/healthz').check('GET').allowed()authorizer.requestResource.check('my<span class="token punctuation">-</span>custom<span class="token punctuation">-</span>verb').allowed()</code></pre><p>在 v1.27 还加入了#115973，该功能允许在失败时作为主要操作发出审计日志事件，或者如果需要更多数据，可以编写一个或多个 CEL 表达式，以提供详细的值， 这些值将发送到审计子系统。</p><p>这既可以在开发新策略时提供强大的调试选项，也可以进行运行时分析。其他 CEL admission 特性包括各种检查，以防止您使用所有这些新功能意外拒绝服务自己的 kube-apiserver，以及改进的类型检查。</p><p>该特性自 v1.28 升级到 Beta，v1.30 升级到 GA 阶段，API version 也就随之升级到了：</p><pre class=" language-yaml"><code class="language-yaml">admissionregistration.k8s.io/v1</code></pre><p>此外，还引入了很多值得关注的特性，包括：</p><ul><li>ValidatingAdmissionPolicy: support namespace access by cici37 · Pull Request #118267 · kubernetes/kubernetes 将 webhook 推入到一个有限的用例空间中（防止滥用）</li><li>CEL-based admission webhook match conditions · Issue #3716 · kubernetes/enhancements 这是另一个 KEP，实现的特性称之为 AdmissionWebhookMatchConditions，也在 v1.30 中达到了 GA，这个特性其实就是个条件判断，在进行请求处理时候多了一份过滤</li></ul><h3 id="基于-container-资源的-HAP-达到-GA"><a href="#基于-container-资源的-HAP-达到-GA" class="headerlink" title="基于 container 资源的 HAP 达到 GA"></a>基于 container 资源的 HAP 达到 GA</h3><p>Horizontal Pod Autoscaler (HPA) 是 Kubernetes 集群中一个非常重要且实现自动伸缩功能必备组件之一，并支持根据目标 Pod 内部所有 Container 所占系统内存、CPU 等信息来判断是否需要进行 HPA 操作。但是，在某些场景下，默认采取“汇总”模式统计所有 Container 节点数据并作为判断依据就不那么适用了：</p><ul><li>比如存在 sidecar 模式下运行额外附属服务进程；</li><li>比如某些业务场景下 CPU 和 Memory 使用完全无法预测、毫无规律可言等特殊状态。<br>针对以上场景，KEP-1610 定义了基于 container 资源的 HAP，扩展了之前基于 Pod 的 HPA 机制。该特性自 v1.19 引入 Alpha ，到 1.27 达到 Beta，在 v1.30 终于 GA。</li></ul><p>同时，在具体操作过程中还需注意以下两点：</p><p>对那些性能极高且需要单独配置各个 container 资源参数（例如 CPU 和 Memory）以达到最优效果业务场景而言，默认 HPA Controller 行为则显得相对简陋；<br>在多数业务场景下，都会需要通过 sidecar 来运行一些额外的服务进程，例如采集日志等。但是由于 sidecar 本身的特殊用途和使用场景，使得它与主业务 container 耦合度较低甚至不存在强关联性，因此统计分析过程中对 sidecar 的资源消耗要独立考虑；</p><h3 id="新的-Service-流量分配机制"><a href="#新的-Service-流量分配机制" class="headerlink" title="新的 Service 流量分配机制"></a>新的 Service 流量分配机制</h3><p>Traffic Distribution for Services · KEP-4444 建议在 Kubernetes service spec 中新增一个名为 trafficDistribution 的字段，以替代目前使用的 service.kubernetes.io/topology-mode annotation 和已于 Kubernetes v1.21 弃用的 topologyKeys 字段。</p><p>具体来说，trafficDistribution 目前支持两个配置：</p><ul><li>nil （默认配置）：没有特别指定任何偏好，用户委派转发决策给底层实现；</li><li>PreferClose：即优先选择与客户端所处拓扑结构相邻接口节点进行流量转发。“拓扑结构相邻”的定义会因不同实现而异，例如可能是同一台主机、机架、区域乃至地理位置内部分接口节点等等。<br>这个特性当前是 Alpha 阶段，未来可能还会有所调整。</li></ul><h3 id="新的-Job-完成-成功策略"><a href="#新的-Job-完成-成功策略" class="headerlink" title="新的 Job 完成/成功策略"></a>新的 Job 完成/成功策略</h3><p>Job success/completion policy KEP-3998 介绍了如何通过扩展 Job API 来设置 Indexed Job 的成功条件。对于某些批处理工作负载（如 MPI 和 PyTorch），我们仅希望考虑 leader 上的结果来判断整个任务是否成功。但现有机制要求所有节点均须执行完毕后方能标记 Indexed job 为完成状态。</p><p>新机制允许用户在 Job 规范中指定 .spec.successPolicy 参数来定义 indexed Jobs 达成成功所需满足的条件。通过该功能可定义两种判定方式：</p><ul><li>succeededIndexes：只要指定的 index 成功了就算整个 Job 成功了（即使有其他 index 失败）</li><li>succeededCount：只有达到指定数量的 index 成功才算 Job 完成<br>一旦 Job 满足完成条件后，Job 控制器会立即停止所有未完成的 Pods。</li></ul><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> batch/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Job<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">parallelism</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">completions</span><span class="token punctuation">:</span> <span class="token number">10</span>  <span class="token key atrule">completionMode</span><span class="token punctuation">:</span> Indexed <span class="token comment" spellcheck="true"># Required for the success policy</span>  <span class="token key atrule">successPolicy</span><span class="token punctuation">:</span>    <span class="token key atrule">rules</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">succeededIndexes</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>2<span class="token punctuation">-</span><span class="token number">3</span>        <span class="token key atrule">succeededCount</span><span class="token punctuation">:</span> <span class="token number">1</span>  <span class="token key atrule">template</span><span class="token punctuation">:</span>    <span class="token key atrule">spec</span><span class="token punctuation">:</span>      <span class="token key atrule">containers</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> main        <span class="token key atrule">image</span><span class="token punctuation">:</span> python        <span class="token key atrule">command</span><span class="token punctuation">:</span>          <span class="token comment" spellcheck="true"># Provided that at least one of the Pods with 0, 2, and 3 indexes has succeeded,</span>                          <span class="token comment" spellcheck="true"># the overall Job is a success.</span>          <span class="token punctuation">-</span> python3          <span class="token punctuation">-</span> <span class="token punctuation">-</span>c          <span class="token punctuation">-</span> <span class="token punctuation">|</span><span class="token scalar string">            import os, sys            if os.environ.get("JOB_COMPLETION_INDEX") == "2":              sys.exit(0)            else:              sys.exit(1)</span></code></pre><p>与之前存在的 complete 和 successCriteriaMet 参数相比较而言：</p><ul><li>complete 表示所有 Pod 均已运行完毕，并且它们全部执行成功或者该任务原本就处于 SuccessCriteriaMet=true 状态；</li><li>而 SuccessCriteriaMet 则表示当前任务至少符合其中一项 successPolicies 定义。<br>也就是说，在某些场景下一个 Task 可能既符合 complete 条件又符合 Success Criteria Met 条件。</li></ul><p>这个特性当前是 Alpha 阶段，而且很明显这个特性出发点是对类似 AI 场景下的任务提供更多支持，也是个方向。</p><h3 id="kubelet-重启后-VolumeManager-重建能力-GA"><a href="#kubelet-重启后-VolumeManager-重建能力-GA" class="headerlink" title="kubelet 重启后 VolumeManager 重建能力 GA"></a>kubelet 重启后 VolumeManager 重建能力 GA</h3><p>Robust VolumeManager reconstruction after kubelet restart KEP-3756 KEP-3756 提出了对 Kubernetes 的 VolumeManager 重构过程的改进，在 kubelet 启动时，通过反转启动过程来实现对已挂载卷的更好的恢复。这个 KEP 的目的是在 kubelet 重启后，使其能够更好地恢复已挂载的卷，并为此提出了一个新的 feature gate NewVolumeManagerReconstruction。</p><p>这个特性最早是 v1.25 引入的，不过当时是作为 SELinuxMountReadWriteOncePod 的一部分进行实现的。后来在 v1.27 达到 Beta，在 1.28 时候默认启用了，现在 v1.30 正式 GA。</p><p>它还是很有用的，尤其是如果需要维护 Kubernetes 集群的话，会更喜欢这个特性。毕竟节点异常/重启后，对卷的相关处理一直都比较痛。</p><h3 id="在卷恢复期间防止未经授权的卷模式转换-GA"><a href="#在卷恢复期间防止未经授权的卷模式转换-GA" class="headerlink" title="在卷恢复期间防止未经授权的卷模式转换 GA"></a>在卷恢复期间防止未经授权的卷模式转换 GA</h3><p>Prevent unauthorised volume mode conversion during volume restore KEP-3141 KEP-3141 提出一种机制来减轻用户在从现有 PVC 创建 VolumeSnapshot 时对卷模式的更改带来的漏洞。</p><p>这个特性更偏向于对现有 Kubernetes 机制的一种加固。</p><p>因为在 Kubernetes v1.20 时有一个 GA 的特性 VolumeSnapshot，用户可以通过使用 VolumeSnapshot 特性，从卷快照创建新的持久化卷。这是非常有用的，比如说 DBA 可以在进行数据库大的变更/迁移操作前，对持久化卷做一次快照，如果出现异常，则可以直接从该快照恢复数据库之前的状态。</p><p>例如通过以下方式可以动态的创建一个快照：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> snapshot.storage.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> VolumeSnapshotClass<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>snapclass<span class="token key atrule">driver</span><span class="token punctuation">:</span> testdriver.csi.k8s.io<span class="token key atrule">deletionPolicy</span><span class="token punctuation">:</span> Delete<span class="token key atrule">parameters</span><span class="token punctuation">:</span>  <span class="token key atrule">csi.storage.k8s.io/snapshotter-secret-name</span><span class="token punctuation">:</span> mysecret  <span class="token key atrule">csi.storage.k8s.io/snapshotter-secret-namespace</span><span class="token punctuation">:</span> mysecretnamespace<span class="token punctuation">---</span><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> snapshot.storage.k8s.io/v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> VolumeSnapshot<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>snapshot  <span class="token key atrule">namespace</span><span class="token punctuation">:</span> ns1<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">volumeSnapshotClassName</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>snapclass  <span class="token key atrule">source</span><span class="token punctuation">:</span>    <span class="token key atrule">persistentVolumeClaimName</span><span class="token punctuation">:</span> test<span class="token punctuation">-</span>pvc</code></pre><p>当然，这里也有些需要注意的点，该功能只能用于 CSI 存储驱动，并且该功能是自 Kubernetes v1.17 达到 beta，v1.20 正式 GA 的。</p><p>VolumeSnapshot 是通过将 PVC 的 spec.dataSource 参数指向现有 VolumeSnapshot 实例来完成的。然而，没有任何逻辑来验证在此操作期间原始卷模式是否与新创建的 PVC 的卷模式匹配。这个 KEP 引入了一个新的 annotation，</p><pre class=" language-yaml"><code class="language-yaml">snapshot.storage.kubernetes.io/allow<span class="token punctuation">-</span>volume<span class="token punctuation">-</span>mode<span class="token punctuation">-</span>change</code></pre><p>用户可以通过它来防止未授权的卷模式转换。</p><h3 id="为-Pod-添加-status-hostIPs-GA"><a href="#为-Pod-添加-status-hostIPs-GA" class="headerlink" title="为 Pod 添加 status.hostIPs GA"></a>为 Pod 添加 status.hostIPs GA</h3><p>Field status.hostIPs added for Pod KEP-2681 该 KEP 提出为 Pod 添加一个新字段 status.hostIP，以获取节点的地址。该 KEP 旨在改善 Pod 获取节点地址的能力，尤其是从单栈向双栈迁移的场景。</p><p>这个特性是 v1.28 引入，到 v1.29 Beta，到 v1.30 GA 并默认启用的。</p><p>你可能会好奇那之前版本中的 status.hostIP 还能用吗？是可以用的，这个 .status.hostIPs 就是为了保证兼容性所以新添加的，它是个列表。效果如下：</p><p>v1.30 之前</p><pre class=" language-shell"><code class="language-shell">➜  ~ kubectl -n kong get pods kong-gateway-649bbf9c47-qk7vh -o=jsonpath='{.status.hostIP}'192.168.1.5</code></pre><p>v1.30</p><pre class=" language-shell"><code class="language-shell">➜  ~ kubectl -n kong get pods kong-gateway-649bbf9c47-qk7vh -o=jsonpath='{.status.hostIPs}'[{"ip":"192.168.1.5"}]</code></pre><h3 id="NodeLogQuery-特性达到-Beta"><a href="#NodeLogQuery-特性达到-Beta" class="headerlink" title="NodeLogQuery 特性达到 Beta"></a>NodeLogQuery 特性达到 Beta</h3><p>我们通常最习惯使用 kubectl logs 命令来获取 Kubernetes 集群中运行的应用的日志，也可以通过给它传递一些参数来调整它的过滤范围。</p><p>比如：kubectl logs job/hello 来指定获取 hello 这个 job 的日志；也可以增加 -l / –selector=’’ 来基于 label 进行过滤。</p><p>但这并没有扩展到 Node 级别的日志，无法直接获取到某个 Node 上的全部日志，要想得到这个结果需要写一段很复杂的 Shell 来完成。</p><p>当前 Kubernetes 社区已经逐渐提供了更多 Node 调试工具，旨在为调试 Node 故障提供统一体验，并更好地支持极简操作系统。通过 KEP 2258: add node log query by LorbusChris · Pull Request #96120 · kubernetes/kubernetes ，我们现在可以远程获取底层 Node 日志。与容器日志记录一样，这是 Kubelet API的一部分。</p><p>对于Linux系统，它查询journald；对于Windows系统，则查询事件日志(Event Log)。目前还没有专门针对此功能的 kubectl 命令，但可以使用如下命令尝试：</p><pre class=" language-shell"><code class="language-shell">➜ kubectl get --raw "/api/v2/nodes/$NODE_NAME/proxy/logs/?query=kubelet</code></pre><p>也有一个对应的 kubectl 插件可以尝试：<a href="https://github.com/aravindhp/kubectl-node-logs/">https://github.com/aravindhp/kubectl-node-logs/</a></p><p>这个特性是自 v1.27 引入的，在 v1.30 达到 Beta，但是默认并没有启用。</p><h3 id="Node-Swap-特性默认启用"><a href="#Node-Swap-特性默认启用" class="headerlink" title="Node Swap 特性默认启用"></a>Node Swap 特性默认启用</h3><p>关于 Node Swap 的背景我在 2021 年 Kubernetes v1.22 的文章中就介绍过了，这个话题是从 2016 年开始就一直在讨论了。</p><p>如今在 v1.30 中默认开始启用，才算是真正的更进一步了。但是需要 注意它的一些参数修改了 ，如果是从之前版本升级，并启用了该特性的话，需要注意！</p><p>具体来说是在 Kubelet 的配置中增加 MemorySwap.SwapBehavior 字段，它有两个可选值：</p><ul><li>NoSwap：它就是之前版本中的 UnlimitedSwap，并且也是默认值；</li><li>LimitedSwap：表示可以使用有限的 Swap，Swap 限制与容器的内存请求成比例；<br>同时，还需要注意，LimitedSwap 仅支持在 cgroup v2 中使用。</li></ul><h3 id="新增递归只读挂载-RRO"><a href="#新增递归只读挂载-RRO" class="headerlink" title="新增递归只读挂载 RRO"></a>新增递归只读挂载 RRO</h3><p>Recursive Read-only (RRO) mounts KEP-3857 它允许在一个目录上创建只读挂载，同时也会递归地将该目录下的所有子目录和文件都设置为只读。</p><p>RRO 挂载可以提高 Kubernetes 的安全性和稳定性，同时也可以减少不必要的读写操作，提高性能。这个功能需要 kernel &gt;= 5.12，并且需要使用 runc &gt;= 1.1 或 crun &gt;= 1.4 之一的 OCI 运行时。</p><p>配置方式如下，主要是增加 recursiveReadOnly: Enabled</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">apiVersion</span><span class="token punctuation">:</span> v1<span class="token key atrule">kind</span><span class="token punctuation">:</span> Pod<span class="token key atrule">metadata</span><span class="token punctuation">:</span>  <span class="token key atrule">name</span><span class="token punctuation">:</span> rro<span class="token key atrule">spec</span><span class="token punctuation">:</span>  <span class="token key atrule">volumes</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mnt      <span class="token key atrule">hostPath</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># tmpfs is mounted on /mnt/tmpfs</span>        <span class="token key atrule">path</span><span class="token punctuation">:</span> /mnt  <span class="token key atrule">containers</span><span class="token punctuation">:</span>    <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> busybox      <span class="token key atrule">image</span><span class="token punctuation">:</span> busybox      <span class="token key atrule">args</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"sleep"</span><span class="token punctuation">,</span> <span class="token string">"infinity"</span><span class="token punctuation">]</span>      <span class="token key atrule">volumeMounts</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># /mnt-rro/tmpfs is not writable</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mnt          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt<span class="token punctuation">-</span>rro          <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>          <span class="token key atrule">mountPropagation</span><span class="token punctuation">:</span> None          <span class="token key atrule">recursiveReadOnly</span><span class="token punctuation">:</span> Enabled        <span class="token comment" spellcheck="true"># /mnt-ro/tmpfs is writable</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mnt          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt<span class="token punctuation">-</span>ro          <span class="token key atrule">readOnly</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>        <span class="token comment" spellcheck="true"># /mnt-rw/tmpfs is writable</span>        <span class="token punctuation">-</span> <span class="token key atrule">name</span><span class="token punctuation">:</span> mnt          <span class="token key atrule">mountPath</span><span class="token punctuation">:</span> /mnt<span class="token punctuation">-</span>rw</code></pre><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><ul><li>移除了 SecurityContextDeny adminssion plugin;</li><li>新增 StorageVersionMigrator 可以方便的进行 CRD 版本升级；</li><li>Pod user namespace 从 v1.25 引入，到如今成为 Beta，但是这需要底层容器运行时支持 user namespace;</li><li>Kubelet ImageMaximumGCAge 达到 Beta;</li><li>Kubelet 可以修改 Pod 的日志目录了，默认是 /var/log/pods 。但是要注意的是，如果你挂了个盘然后它和 /var 不是一个文件系统的话，kubelet 不会感知到文件系统的用量，可能会导致一些异常情况；</li></ul><p>好了，这就是我觉得 Kubernetes v1.30 中主要值得关注的内容了。欢迎大家交流讨论，下次再见！</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubeadm安装k8s集群</title>
      <link href="/2024/03/18/kubeadm-an-zhuang-k8s-ji-qun/"/>
      <url>/2024/03/18/kubeadm-an-zhuang-k8s-ji-qun/</url>
      
        <content type="html"><![CDATA[<h1 id="kubeadm安装k8s集群"><a href="#kubeadm安装k8s集群" class="headerlink" title="kubeadm安装k8s集群"></a>kubeadm安装k8s集群</h1><h2 id="安装kubeadm准备工作"><a href="#安装kubeadm准备工作" class="headerlink" title="安装kubeadm准备工作"></a>安装kubeadm准备工作</h2><h2 id="安装工具"><a href="#安装工具" class="headerlink" title="安装工具"></a>安装工具</h2><pre class=" language-shell"><code class="language-shell"># 配置镜像cat << EOF | tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装工具yum install -y kubeadm kubelet kubectl --enablerepo=kubernetes</code></pre><h2 id="containerd安装"><a href="#containerd安装" class="headerlink" title="containerd安装"></a>containerd安装</h2><pre class=" language-shell"><code class="language-shell">wget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install containerd.io</code></pre><pre class=" language-shell"><code class="language-shell"># cni指定cat << EOF | tee /etc/cni/net.d/10-containerd-net.conflist{ "cniVersion": "1.0.0", "name": "containerd-net", "plugins": [   {     "type": "bridge",     "bridge": "cni0",     "isGateway": true,     "ipMasq": true,     "promiscMode": true,     "ipam": {       "type": "host-local",       "ranges": [         [{           "subnet": "10.88.0.0/16"         }],         [{           "subnet": "2001:db8:4860::/64"         }]       ],       "routes": [         { "dst": "0.0.0.0/0" },         { "dst": "::/0" }       ]     }   },   {     "type": "portmap",     "capabilities": {"portMappings": true},     "externalSetMarkChain": "KUBE-MARK-MASQ"   } ]}EOF# 配置路由转发echo 1 >/proc/sys/net/bridge/bridge-nf-call-iptablesecho 1 >/proc/sys/net/ipv4/ip_forward</code></pre><p>初始化集群</p><pre class=" language-shell"><code class="language-shell">kubeadm init --v=5 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and keyI0318 17:13:06.333130   30977 kubeletfinalize.go:135] [kubelet-finalize] Restarting the kubelet to enable client certificate rotation[addons] Applied essential addon: CoreDNS[addons] Applied essential addon: kube-proxyYour Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run:  export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 10.100.60.28:6443 --token 8upfhx.vl9d3m5myie3t89m \    --discovery-token-ca-cert-hash sha256:8a34de03c83faeead828e6e5349cb66f597bb48f463384cdc21f296b253c88c3使用kubectl proxykubectl proxy --port=8080 &</code></pre><h3 id="如果不小心没保存初始化成功的输出信息也没有关系，我们可以使用kubectl工具查看或者生成token"><a href="#如果不小心没保存初始化成功的输出信息也没有关系，我们可以使用kubectl工具查看或者生成token" class="headerlink" title="如果不小心没保存初始化成功的输出信息也没有关系，我们可以使用kubectl工具查看或者生成token"></a>如果不小心没保存初始化成功的输出信息也没有关系，我们可以使用kubectl工具查看或者生成token</h3><pre class=" language-shell"><code class="language-shell"># 查看现有的token列表$ kubeadm token listTOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPSabcdef.0123456789abcdef   23h         2022-05-08T06:27:34Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token# 如果token已经失效，那就再创建一个新的token$ kubeadm token createpyab3u.j1a9ld7vk03znbk8$ kubeadm token listTOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPSabcdef.0123456789abcdef   23h         2022-05-08T06:27:34Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-tokenpyab3u.j1a9ld7vk03znbk8   23h         2022-05-08T06:34:28Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token# 如果找不到--discovery-token-ca-cert-hash参数，则可以在master节点上使用openssl工具来获取$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'d6cdc5a3bc40cbb0ae85776eb4fcdc1854942e2dd394470ae0f2f97714dd9fb9</code></pre><h3 id="删除node节点"><a href="#删除node节点" class="headerlink" title="删除node节点"></a>删除node节点</h3><pre class=" language-shell"><code class="language-shell">kubectl delete node 节点名称#在节点上执行systemctl stop kubeletsystemctl stop containerdrm -rf /var/lib/cni/rm -rf /var/lib/kubelet/*rm -rf /etc/cni/ifconfig flannel.1 downip link delete flannel.1</code></pre><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="1-提示CRI异常？"><a href="#1-提示CRI异常？" class="headerlink" title="1. 提示CRI异常？"></a>1. 提示CRI异常？</h3><pre class=" language-shell"><code class="language-shell">kubeadm init[init] Using Kubernetes version: v1.29.3[preflight] Running pre-flight checkserror execution phase preflight: [preflight] Some fatal errors occurred:    [ERROR CRI]: container runtime is not running: output: time="2024-03-18T16:26:13+08:00" level=fatal msg="validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService", error: exit status 1[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`To see the stack trace of this error execute with --v=5 or higher# 查看containerd版本containerd -vcontainerd github.com/containerd/containerd 1.2.4# 更新containerd版本``` shellwget https://github.com/containerd/containerd/releases/download/v1.7.14/containerd-1.7.14-linux-amd64.tar.gztar -zxvf containerd-1.7.14-linux-amd64.tar.gzmv bin/* /usr/local/bin/#配置systemd文件vi /etc/systemd/system/containerd.service# Copyright The containerd Authors.## Licensed under the Apache License, Version 2.0 (the "License");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at##     http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an "AS IS" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.[Unit]Description=containerd container runtimeDocumentation=https://containerd.ioAfter=network.target local-fs.target[Service]ExecStartPre=-/sbin/modprobe overlayExecStart=/usr/local/bin/containerdType=notifyDelegate=yesKillMode=processRestart=alwaysRestartSec=5# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNPROC=infinityLimitCORE=infinity# Comment TasksMax if your systemd version does not supports it.# Only systemd 226 and above support this version.TasksMax=infinityOOMScoreAdjust=-999[Install]WantedBy=multi-user.targetsystemctl daemon-reloadsystemctl restart containerdsystemctl status containerd# 查看containerd版本containerd -vcontainerd github.com/containerd/containerd v1.7.14 dcf2847247e18caba8dce86522029642f60fe96b</code></pre><h3 id="2-coredns安装失败"><a href="#2-coredns安装失败" class="headerlink" title="2. coredns安装失败"></a>2. coredns安装失败</h3><pre class=" language-shell"><code class="language-shell">kubectl get pod -ANAMESPACE     NAME                                              READY   STATUS              RESTARTS      AGEkube-system   coredns-5f98f8d567-2c4g9                          0/1     Terminating         0             61mkube-system   coredns-5f98f8d567-6c6sg                          0/1     ContainerCreating   0             38mkube-system   coredns-5f98f8d567-h572m                          0/1     ContainerCreating   0             61mkubectl describe pod coredns-5f98f8d567-6c6sg -n kube-system...Events:  Type     Reason                  Age                  From               Message  ----     ------                  ----                 ----               -------  Warning  FailedScheduling        33m                  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.  Warning  FailedScheduling        27m                  default-scheduler  0/2 nodes are available: 2 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.  Normal   Scheduled               25m                  default-scheduler  Successfully assigned kube-system/coredns-5f98f8d567-6c6sg to tx7-ops-beian-temp81.bj  Warning  FailedCreatePodSandBox  25m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox "377bcf7984f12632bf9c55704f85744283ce8b75f6396f815e17a1548181d236": plugin type="flannel" failed (add): failed to find plugin "flannel" in path [/opt/cni/bin]  Normal   SandboxChanged          17s (x117 over 25m)  kubelet            Pod sandbox changed, it will be killed and re-created.</code></pre><p>指定的cni找不到 <a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/#updating-your-cni-plugins-and-cni-config-files">官方文档</a></p><pre class=" language-shell"><code class="language-shell">cat << EOF | tee /etc/cni/net.d/10-containerd-net.conflist{ "cniVersion": "1.0.0", "name": "containerd-net", "plugins": [   {     "type": "bridge",     "bridge": "cni0",     "isGateway": true,     "ipMasq": true,     "promiscMode": true,     "ipam": {       "type": "host-local",       "ranges": [         [{           "subnet": "10.88.0.0/16"         }],         [{           "subnet": "2001:db8:4860::/64"         }]       ],       "routes": [         { "dst": "0.0.0.0/0" },         { "dst": "::/0" }       ]     }   },   {     "type": "portmap",     "capabilities": {"portMappings": true},     "externalSetMarkChain": "KUBE-MARK-MASQ"   } ]}EOF</code></pre><p>重启containerd</p><h3 id="3-提示kubelet超时"><a href="#3-提示kubelet超时" class="headerlink" title="3. 提示kubelet超时"></a>3. 提示kubelet超时</h3><pre class=" language-shell"><code class="language-shell">...[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"[kubelet-start] Starting the kubeletI0318 17:07:14.119153   18564 waitcontrolplane.go:83] [wait-control-plane] Waiting for the API server to be healthy[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s[kubelet-check] Initial timeout of 40s passed.Unfortunately, an error has occurred:    timed out waiting for the conditionThis error is likely caused by:    - The kubelet is not running    - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:    - 'systemctl status kubelet'    - 'journalctl -xeu kubelet'Additionally, a control plane component may have crashed or exited when started by the container runtime.# 启动kubelet超时 # 主机名解析配置cat /etc/hosts</code></pre><h3 id="node-cni异常"><a href="#node-cni异常" class="headerlink" title="node cni异常"></a>node cni异常</h3><pre><code>kubectl describe node tx7-ops-beian-temp82.bjConditions:  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message  ----             ------  -----------------                 ------------------                ------                       -------  MemoryPressure   False   Mon, 18 Mar 2024 18:36:14 +0800   Mon, 18 Mar 2024 18:31:38 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available  DiskPressure     False   Mon, 18 Mar 2024 18:36:14 +0800   Mon, 18 Mar 2024 18:31:38 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure  PIDPressure      False   Mon, 18 Mar 2024 18:36:14 +0800   Mon, 18 Mar 2024 18:31:38 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available  Ready            False   Mon, 18 Mar 2024 18:36:14 +0800   Mon, 18 Mar 2024 18:31:38 +0800   KubeletNotReady              container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initializedAddresses:...</code></pre><p>查看Node节点kubelet日志</p><pre class=" language-shell"><code class="language-shell">journalctl -u kubeletkuberuntime_manager.go:258] "Container runtime initialized" containerRuntime="containerd" version="v1.7.14" apiVersion="v1"</code></pre><p>解决方法跟 <a href="###2">问题2</a></p><h3 id="配置路由转发失败"><a href="#配置路由转发失败" class="headerlink" title="配置路由转发失败"></a>配置路由转发失败</h3><pre class=" language-shell"><code class="language-shell">    [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist</code></pre><p>解决方法：</p><pre class=" language-shell"><code class="language-shell">modprobe br_netfilterecho 1 > /proc/sys/net/bridge/bridge-nf-call-iptables</code></pre><p>kubeadm join 10.100.60.67:6443 –token nuluvn.go8gi8wcbd38pt8j <br>    –discovery-token-ca-cert-hash sha256:e6567fd2a447002313dcf7de669d6811045e191d61e005da7fdc256bebc8295d</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubeadm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>替代ELK：ClickHouse+Kafka+FlieBeat，才是最佳选择？</title>
      <link href="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/"/>
      <url>/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/</url>
      
        <content type="html"><![CDATA[<h1 id="替代ELK：ClickHouse-Kafka-FlieBeat，才是最佳选择？"><a href="#替代ELK：ClickHouse-Kafka-FlieBeat，才是最佳选择？" class="headerlink" title="替代ELK：ClickHouse+Kafka+FlieBeat，才是最佳选择？"></a>替代ELK：ClickHouse+Kafka+FlieBeat，才是最佳选择？</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>SaaS 服务未来会面临数据安全、合规等问题。公司的业务需要沉淀一套私有化部署能力，帮助业务提升行业竞争力。为了完善平台系统能力、我们需要沉淀一套数据体系帮助运营分析活动效果、提升运营能力。</p><p>然而在实际的开发过程中，如果直接部署一套大数据体系，对于使用者来说将是一笔比较大的服务器开销。为此我们选用折中方案完善数据分析能力。</p><p>Easticsearch vs Clickhouse<br>ClickHouse是一款高性能列式分布式数据库管理系统，我们对ClickHouse进行了测试，发现有下列优势：</p><p>ClickHouse写入吞吐量大，单服务器日志写入量在50MB到200MB/s，每秒写入超过60w记录数，是ES的5倍以上。在ES中比较常见的写Rejected导致数据丢失、写入延迟等问题，在ClickHouse中不容易发生。</p><p>查询速度快，官方宣称数据在pagecache中，单服务器查询速率大约在2-30GB/s；没在pagecache的情况下，查询速度取决于磁盘的读取速率和数据的压缩率。经测试ClickHouse的查询速度比ES快5-30倍以上。</p><p>ClickHouse比ES服务器成本更低。一方面ClickHouse的数据压缩比比ES高，相同数据占用的磁盘空间只有ES的1/3到1/30，节省了磁盘空间的同时，也能有效的减少磁盘IO，这也是ClickHouse查询效率更高的原因之一；另一方面ClickHouse比ES占用更少的内存，消耗更少的CPU资源。我们预估用ClickHouse处理日志可以将服务器成本降低一半。</p><p><img src="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/image.png" alt="架构"></p><p><img src="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/image-1.png" alt="成本对比"></p><p>成本分析<br>备注：在没有任何折扣的情况下，基于aliyun分析</p><p><img src="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/image-2.png" alt="成本分析"></p><h2 id="环境部署"><a href="#环境部署" class="headerlink" title="环境部署"></a>环境部署</h2><h3 id="zookeeper-集群部署"><a href="#zookeeper-集群部署" class="headerlink" title="zookeeper 集群部署"></a>zookeeper 集群部署</h3><p><img src="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/image-3.png" alt="启动zk"></p><pre class=" language-shell"><code class="language-shell">yum install java-1.8.0-openjdk-devel.x86_64#将java环境变量加入/etc/profile 配置环境变量#更新系统时间yum install  ntpdatentpdate asia.pool.ntp.orgmkdir zookeepermkdir ./zookeeper/datamkdir ./zookeeper/logswget  --no-check-certificate https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gztar -zvxf apache-zookeeper-3.7.1-bin.tar.gz -C /usr/zookeeperexport ZOOKEEPER_HOME=/usr/zookeeper/apache-zookeeper-3.7.1-binexport PATH=$ZOOKEEPER_HOME/bin:$PATH#进入ZooKeeper配置目录cd $ZOOKEEPER_HOME/conf#新建配置文件vim zoo.cfgtickTime=2000initLimit=10syncLimit=5dataDir=/usr/zookeeper/datadataLogDir=/usr/zookeeper/logsclientPort=2181server.1=zk1:2888:3888server.2=zk2:2888:3888server.3=zk3:2888:3888# 在每台服务器上执行，给zookeeper创建myidecho "1" > /usr/zookeeper/data/myidecho "2" > /usr/zookeeper/data/myidecho "3" > /usr/zookeeper/data/myid#进入ZooKeeper bin目录cd $ZOOKEEPER_HOME/binsh zkServer.sh start</code></pre><h3 id="Kafka-集群部署"><a href="#Kafka-集群部署" class="headerlink" title="Kafka 集群部署"></a>Kafka 集群部署</h3><pre class=" language-shell"><code class="language-shell">mkdir -p /usr/kafkachmod 777 -R /usr/kafkawget  --no-check-certificate https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/3.2.0/kafka_2.12-3.2.0.tgztar -zvxf kafka_2.12-3.2.0.tgz -C /usr/kafka#不同的broker Id 设置不一样，比如 1,2,3broker.id=1listeners=PLAINTEXT://ip:9092socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dir=/usr/kafka/logsnum.partitinotallow=5num.recovery.threads.per.data.dir=3offsets.topic.replication.factor=2transaction.state.log.replication.factor=3transaction.state.log.min.isr=3log.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.cnotallow=zk1:2181,zk2:2181,zk3:2181zookeeper.connection.timeout.ms=30000group.initial.rebalance.delay.ms=0#后台常驻进程启动kafkanohup /usr/kafka/kafka_2.12-3.2.0/bin/kafka-server-start.sh /usr/kafka/kafka_2.12-3.2.0/config/server.properties   >/usr/kafka/logs/kafka.log >&1 &/usr/kafka/kafka_2.12-3.2.0/bin/kafka-server-stop.sh$KAFKA_HOME/bin/kafka-topics.sh --list --bootstrap-server  ip:9092$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server ip:9092 --topic test --from-beginning$KAFKA_HOME/bin/kafka-topics.sh  --create --bootstrap-server  ip:9092  --replication-factor 2 --partitions 3 --topic xxx_data</code></pre><h3 id="FileBeat-部署"><a href="#FileBeat-部署" class="headerlink" title="FileBeat 部署"></a>FileBeat 部署</h3><pre class=" language-shell"><code class="language-shell">sudo rpm --import https://packages.elastic.co/GPG-KEY-elasticsearchCreate a file with a .repo extension (for example, elastic.repo) in your /etc/yum.repos.d/ directory and add the following lines:# 在/etc/yum.repos.d/ 目录下创建elastic.repo[elastic-8.x]name=Elastic repository for 8.x packagesbaseurl=https://artifacts.elastic.co/packages/8.x/yumgpgcheck=1gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearchenabled=1autorefresh=1type=rpm-mdyum install filebeatsystemctl enable filebeatchkconfig --add filebeat</code></pre><p>FileBeat 配置文件说明，坑点1（需设置keys_under_root: true）。如果不设置kafka的消息字段如下：</p><p><img src="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/image-4.png" alt="filebeat字段说明"></p><p>文件目录：/etc/filebeat/filebeat.yml</p><pre class=" language-yml"><code class="language-yml">filebeat.inputs:- type: log  enabled: true  paths:    - /root/logs/xxx/inner/*.log  json:  # 如果不设置该索性，所有的数据都存储在message里面，这样设置以后数据会平铺。       keys_under_root: true output.kafka:  hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]  topic: 'xxx_data_clickhouse'  partition.round_robin:            reachable_only: false            required_acks: 1            compression: gzipprocessors: # 剔除filebeat 无效的字段数据    - drop_fields:          fields: ["input", "agent", "ecs", "log", "metadata", "timestamp"]        ignore_missing: falsenohup ./filebeat -e -c /etc/filebeat/filebeat.yml > /user/filebeat/filebeat.log & </code></pre><p>输出到filebeat.log文件中，方便排查</p><h3 id="Clickhouse-部署"><a href="#Clickhouse-部署" class="headerlink" title="Clickhouse 部署"></a>Clickhouse 部署</h3><p><img src="/2024/03/15/ti-dai-elk-clickhouse-kafka-fliebeat-cai-shi-zui-jia-xuan-ze/image-5.png" alt="Alt text"></p><pre class=" language-shell"><code class="language-shell">#检查当前CPU是否支持SSE 4.2，如果不支持，需要通过源代码编译构建grep -q sse4_2 /proc/cpuinfo && echo "SSE 4.2 supported" || echo "SSE 4.2 not supported"# 返回 "SSE 4.2 supported" 表示支持，返回 "SSE 4.2 not supported" 表示不支持# 创建数据保存目录，将它创建到大容量磁盘挂载的路径mkdir -p /data/clickhouse# 修改/etc/hosts文件，添加clickhouse节点# 举例：10.190.85.92 bigdata-clickhouse-0110.190.85.93 bigdata-clickhouse-02# 服务器性能参数设置：# cpu频率调节，将CPU频率固定工作在其支持的最高运行频率上，而不动态调节，性能最好echo 'performance' | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor# 内存调节，不要禁用 overcommitecho 0 | tee /proc/sys/vm/overcommit_memory# 始终禁用透明大页(transparent huge pages)。它会干扰内存分配器，从而导致显着的性能下降echo 'never' | tee /sys/kernel/mm/transparent_hugepage/enabled# 首先，需要添加官方存储库：yum install yum-utilsrpm --import <https://repo.clickhouse.tech/CLICKHOUSE-KEY.GPG>yum-config-manager --add-repo <https://repo.clickhouse.tech/rpm/stable/x86_64># 查看clickhouse可安装的版本：yum list | grep clickhouse# 运行安装命令：yum -y install clickhouse-server clickhouse-client# 修改/etc/clickhouse-server/config.xml配置文件，修改日志级别为information，默认是trace<level>information</level># 执行日志所在目录：# 正常日志/var/log/clickhouse-server/clickhouse-server.log# 异常错误日志/var/log/clickhouse-server/clickhouse-server.err.log# 查看安装的clickhouse版本：clickhouse-server --versionclickhouse-client --passwordsudo clickhouse stopsudo clickhouse tartsudo clickhouse start</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整个部署的过程踩了不少坑，尤其是filebeat yml的参数设置。希望这篇文章可以给你一些建议</p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ClickHouse </tag>
            
            <tag> ELK </tag>
            
            <tag> 日志收集 </tag>
            
            <tag> 日志可观测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于MiniKube安装Kubernetes</title>
      <link href="/2024/02/27/guo-nei-ji-yu-minikube-an-zhuang-kubernetes/"/>
      <url>/2024/02/27/guo-nei-ji-yu-minikube-an-zhuang-kubernetes/</url>
      
        <content type="html"><![CDATA[<h1 id="国内基于minikube安装kubernetes"><a href="#国内基于minikube安装kubernetes" class="headerlink" title="国内基于minikube安装kubernetes"></a>国内基于minikube安装kubernetes</h1><pre><code>个人学习kubernets练手，minikube可以满足基本的安装环境，方便快速学习机器要求至少2CPU 2G内存，硬盘20G，安装过程需要网络实验环境Centos7.6 </code></pre><h3 id="配置阿里镜像源"><a href="#配置阿里镜像源" class="headerlink" title="配置阿里镜像源"></a>配置阿里镜像源</h3><pre class=" language-shell"><code class="language-shell">wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/docker-ce.repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</code></pre><h3 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h3><pre class=" language-shell"><code class="language-shell">yum install -y yum-utils   device-mapper-persistent-data   lvm2yum-config-manager     --add-repo     http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install docker-ce-20.10.6</code></pre><h4 id="docker国内源"><a href="#docker国内源" class="headerlink" title="docker国内源"></a>docker国内源</h4><pre class=" language-shell"><code class="language-shell">cat <<EOF > /etc/docker/daemon.json{  "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn/"]}EOF</code></pre><h4 id="启动docker"><a href="#启动docker" class="headerlink" title="启动docker"></a>启动docker</h4><pre class=" language-shell"><code class="language-shell">systemctl start dockersystemctl enable docker</code></pre><p>###安装minikube <a href="https://minikube.sigs.k8s.io/docs/start/">官网地址</a> </p><pre class=" language-shell"><code class="language-shell">curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64sudo install minikube-linux-amd64 /usr/local/bin/minikube</code></pre><h3 id="minikube启动kubernetes集群"><a href="#minikube启动kubernetes集群" class="headerlink" title="minikube启动kubernetes集群"></a>minikube启动kubernetes集群</h3><pre class=" language-shell"><code class="language-shell">minikube start --driver=docker --image-mirror-country=cn --force</code></pre><p><img src="/2024/02/27/guo-nei-ji-yu-minikube-an-zhuang-kubernetes/image.png" alt="安装过程"><br><img src="/2024/02/27/guo-nei-ji-yu-minikube-an-zhuang-kubernetes/image-1.png" alt="集群验证"></p><h3 id="安装kubectl"><a href="#安装kubectl" class="headerlink" title="安装kubectl"></a>安装kubectl</h3><p>minikube安装kubernetes集群需要使用命令minikube kubectl 命令操作，为了操作习惯，这里安装kubectl命令</p><pre class=" language-shell"><code class="language-shell">cat <<EOF > /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install kubectl -y</code></pre><p><img src="/2024/02/27/guo-nei-ji-yu-minikube-an-zhuang-kubernetes/image-2.png" alt="kubectl命令"></p>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> MiniKube </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github Pages创建自己域名网站</title>
      <link href="/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/"/>
      <url>/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/</url>
      
        <content type="html"><![CDATA[<h2 id="一、准备工作"><a href="#一、准备工作" class="headerlink" title="一、准备工作"></a>一、准备工作</h2><h3 id="1-1-注册域名"><a href="#1-1-注册域名" class="headerlink" title="1.1 注册域名"></a>1.1 注册域名</h3><p>自行到各个域名注册商注册域名即可，腾讯云，阿里云都支持</p><h3 id="1-2-注册github账号"><a href="#1-2-注册github账号" class="headerlink" title="1.2 注册github账号"></a>1.2 注册github账号</h3><p><a href="https://github.com/">Github</a>账号注册</p><h3 id="1-3-安装git环境"><a href="#1-3-安装git环境" class="headerlink" title="1.3 安装git环境"></a>1.3 安装git环境</h3><p>根据</p><h2 id="二、创建github-pages"><a href="#二、创建github-pages" class="headerlink" title="二、创建github pages"></a>二、创建github pages</h2><p>创建仓库，仓库名称为站点域名 blog.daydayops.com</p><p><img src="/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/create_repo.png" alt="创建仓库"></p><h3 id="2-1-创建github-pages"><a href="#2-1-创建github-pages" class="headerlink" title="2.1 创建github pages"></a>2.1 创建github pages</h3><p>仓库–&gt;settings–&gt;pages<br><img src="/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/github_page.png" alt="创建github pages"></p><h3 id="2-2-配置域名"><a href="#2-2-配置域名" class="headerlink" title="2.2 配置域名"></a>2.2 配置域名</h3><p>配置域名解析blog.daydayops.com CNAME   username.github.io<br><img src="/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/cname.png" alt="配置域名"></p><h2 id="三、基于hexo创建网站"><a href="#三、基于hexo创建网站" class="headerlink" title="三、基于hexo创建网站"></a>三、基于hexo创建网站</h2><p>Hexo 是一个基于 Node.js 的静态博客框架，它允许用户使用简单的 Markdown 语法编写文章，并能快速生成静态网页。</p><h3 id="3-1-安装hexo"><a href="#3-1-安装hexo" class="headerlink" title="3.1 安装hexo"></a>3.1 安装hexo</h3><pre class=" language-shell"><code class="language-shell">npm install hexo -g</code></pre><h3 id="3-2-创建hexo"><a href="#3-2-创建hexo" class="headerlink" title="3.2 创建hexo"></a>3.2 创建hexo</h3><pre class=" language-shell"><code class="language-shell">hexo init daydayops_blogcd daydayops_blognpm install</code></pre><h3 id="3-3-主题选择"><a href="#3-3-主题选择" class="headerlink" title="3.3 主题选择"></a>3.3 主题选择</h3><p><a href="https://hexo.io/themes/">官网</a>有很多主题，这里选择<a href="https://blinkfox.github.io/">matery</a><br>将主题源码clone到themes目录下</p><pre class=" language-shell"><code class="language-shell">cd daydayops_blog/themesgit clone https://github.com/blinkfox/hexo-theme-matery.git</code></pre><p>具体详细配置可以<a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/">参考文章</a></p><h2 id="四、部署"><a href="#四、部署" class="headerlink" title="四、部署"></a>四、部署</h2><h3 id="4-1-本地启动"><a href="#4-1-本地启动" class="headerlink" title="4.1 本地启动"></a>4.1 本地启动</h3><pre class=" language-shell"><code class="language-shell">hexo s</code></pre><p>访问本地地址<a href="http://localhost:4000/">http://localhost:4000/</a></p><h3 id="4-2-生成静态文件"><a href="#4-2-生成静态文件" class="headerlink" title="4.2 生成静态文件"></a>4.2 生成静态文件</h3><pre class=" language-shell"><code class="language-shell">hexo g</code></pre><h3 id="4-3-配置github上传仓库"><a href="#4-3-配置github上传仓库" class="headerlink" title="4.3 配置github上传仓库"></a>4.3 配置github上传仓库</h3><p>安装hexo-deployer-git</p><pre class=" language-shell"><code class="language-shell">npm install hexo-deployer-git --save</code></pre><p>修改根目录下的 _config.yml，配置 GitHub 相关信息</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">deploy</span><span class="token punctuation">:</span>  <span class="token key atrule">type</span><span class="token punctuation">:</span> git  <span class="token key atrule">repo</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//github.com/daydayops/blog.daydayops.com.git  <span class="token key atrule">branch</span><span class="token punctuation">:</span> main  <span class="token key atrule">message</span><span class="token punctuation">:</span> <span class="token string">'hexo deploy'</span>  <span class="token key atrule">token</span><span class="token punctuation">:</span> xxx</code></pre><p>其中 token 为 GitHub 的 Personal access tokens，获取方式如下图<br><img src="/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/image.png" alt="Alt text"></p><h3 id="4-4-上传到github"><a href="#4-4-上传到github" class="headerlink" title="4.4 上传到github"></a>4.4 上传到github</h3><pre class=" language-shell"><code class="language-shell">hexo d</code></pre><p>部署完成<br><img src="/2024/01/31/github-pages-chuang-jian-zi-ji-yu-ming-wang-zhan/image-1.png" alt="Alt text"></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="1-CNAME问题"><a href="#1-CNAME问题" class="headerlink" title="1. CNAME问题"></a>1. CNAME问题</h3><p>配置github pages后github默认会在仓库里新增一个CNAME文件，文件里是自己要解析域名，但是每次hexo上传代码后，这个文件就会被覆盖，导致域名解析失败，所以需要手动修改CNAME文件，将域名写入CNAME文件中，这样每次hexo上传代码后，CNAME文件不会被覆盖，域名解析也不会失败。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>在source目录下创建CNAME文件，文件内容为自己的域名，如blog.daydayops.com。</p><h3 id="2-代码显示问题"><a href="#2-代码显示问题" class="headerlink" title="2. 代码显示问题"></a>2. 代码显示问题</h3><p>hexo7.1.1版本，该主题下代码块显示不正常，网上的多数办法不行</p><h3 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法"></a>解决方法</h3><p>在_config.yml文件中添加如下配置：</p><pre class=" language-yaml"><code class="language-yaml"><span class="token key atrule">syntax_highlighter</span><span class="token punctuation">:</span> prismjs<span class="token key atrule">highlight</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">line_number</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">auto_detect</span><span class="token punctuation">:</span> <span class="token boolean important">false</span>  <span class="token key atrule">tab_replace</span><span class="token punctuation">:</span> <span class="token string">''</span>  <span class="token key atrule">wrap</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">hljs</span><span class="token punctuation">:</span> <span class="token boolean important">false</span><span class="token key atrule">prismjs</span><span class="token punctuation">:</span>  <span class="token key atrule">enable</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">preprocess</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">line_number</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>  <span class="token key atrule">tab_replace</span><span class="token punctuation">:</span> <span class="token string">''</span></code></pre><p>替换prismjs的的css,js文件，<a href="https://github.com/blinkfox/hexo-theme-matery/issues/910">参考链接</a></p><p>配置参考链接：</p><p><a href="https://blog.csdn.net/yaorongke/article/details/119089190">https://blog.csdn.net/yaorongke/article/details/119089190</a></p><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/">https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/</a></p>]]></content>
      
      
      <categories>
          
          <category> web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GithubPages </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
